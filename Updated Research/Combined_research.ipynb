{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a181cd30",
   "metadata": {},
   "source": [
    "# Spam Detection Project Reproduced One-to-One"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b60d81aa",
   "metadata": {},
   "source": [
    "# Reproducible Research Project\n",
    "\n",
    "### Jakub Cudak\n",
    "### Paulina Sereikyte\n",
    "### Dawid Szyszko-Celinski\n",
    "\n",
    "![alt text](https://www.netia.pl/Netia/media/blog/spam-co-to-jest.png)\n",
    "\n",
    "Source: https://www.netia.pl/Netia/media/blog/spam-co-to-jest.png\n",
    "\n",
    "## Aim of the initial project\n",
    "\n",
    "The main aim of the project is to reproduce the econometric project submitted for the \"Advanced Econometric\" subject. The initial project idea was to create a logit and probit econometric model in RStudio that would predict if the email is spam. The dataset used for the initial analysis mainly consisted of certain keywords frequenies in the text and additional metrics such as a number of words in the email.\n",
    "\n",
    "The approach of the modelling was as follows:\n",
    "1) Create a basic probit and logit model that consists of all explanatory variables\n",
    "2) Perform LR (likelihood Ratio) test to verify if the model`s parameters are jointly significant\n",
    "3) Perform stepwise regression for both models to remove all coefficients whose p-value is above the 5% threshold\n",
    "4) Again perform the LR test and additional Link test that would check if the model has a good specification\n",
    "5) Add interaction between variables to the analysis\n",
    "6) Perform stepwise regression\n",
    "7) Execute LR and Link test\n",
    "8) Execute the Hosmer-Lemershow test and Osjus-Rojek test to verify the specification\n",
    "9) Verify goodness of fit statistics\n",
    "10) Calculate marginal effects\n",
    "\n",
    "The whole analysis and report can be found in the \"Initial research\" file:\n",
    "1) The \"Advanced Econometrics 2022 project\" contains a report and description of the steps of the analysis\n",
    "2) \"spambase - raw data\" consists of the raw dataset used for initial analysis\n",
    "3) \"Spam detection codes\" - consist of R script used to perform the analysis\n",
    "4) \"names.csv\" is an updated file with variables names, that can be read by R studio in CSV format\n",
    "5) \"linktest.R\" and \"marginaleffects.R\" are custom functions written in R by dr Rafal Wozniak, Faculty of Economic Sciences, University of Warsaw\n",
    "\n",
    "## Aim of this project\n",
    "\n",
    "The aim of this project is to try reproducing the initial research in a different programming language - Python. We would like to verify if the actions performed in R can be easily translated into Python codes. We would like to verify if the outcomes are the same or if they differ due to possibly different algorithms that could be used in certain functions.\n",
    "\n",
    "What is more, we would like to perform the same analysis, but on a different dataset with the same keyword that was used in the initial research. Based on the new dataset new keywords will be also obtained and a new model will be produced to verify how probit and logit models will work on the different datasets and keywords. The new dataset that will be used contains only text and labels if the message is spam or not, which is why it will be necessary to perform some data transformations. The process of the initial research will be followed.\n",
    "\n",
    "In this part of the analysis we will try to recreate the previous research on the new data directly. We use an Email Spam Detection Dataset, which is available on Kaggle, and can be accessed via: https://www.kaggle.com/datasets/shantanudhakadd/email-spam-detection-dataset-classification\n",
    "\n",
    "The dataset contains 5169 unique observations, out of which 87% are non-spam emails, while only 13% are classified as spam. While split is more challenging in terms of training our models, we can expect results to be less accurate. The e-mails in the dataset are not\n",
    "\n",
    "The whole analysis can be found in the \"Updated Research\" file:\n",
    "1) \"Combined research\" consists of a report and codes in jupyter notebook that contains all codes necessary to recreate initial research one-to-one and 2 updated versions: with old and new keywords on the new dataset.\n",
    "2) \"new_data_new_words.csv\" dataset that will be used to analyse new keywords on a new dataset\n",
    "3) \"new_data_old_words.csv\" dataset that will be used to analyse old keywords on a new dataset\n",
    "4) \"Technical\" folder that contains all codes that were used as technical parts to create the main file \"Combined research\"\n",
    "5) \"Link to the dataset\" - a text file containing a link to the new dataset\n",
    "\n",
    "## General results\n",
    "\n",
    "We were able to replicate the analysis, which was originally designed in R, in Python. Some adjustments had to be made to create new formulas and utilise a different set of packages, however the results generated in Python were the same as the results generated in R when using the same dataset.\n",
    "\n",
    "Applying the same methodologies to a new dataset were more challenging, as the initial analysis was based on the word frequencies of certain words in the text. At first, the analysis was conducted based on the most common words and characters present in the new dataset. Then the models were replicated by calculating the frequencies of the words from the initial word list. \n",
    "While it was still possible to construct the models, they were generally less efficient in detecting spam. One of the main reasons was the much lower frequencies of the selected and previously used words in the new text. This resulted in very low variances within the variable which then had to be removed to be included in the models.\n",
    "This issue was more prevalent when trying to replicate the analysis using the old word list - many of the words were highly specific to the initial dataset and thus were not found in the new data. This resulted in a big reduction in the number of variables and possible interactions, therefore the analysis could not be replicated exactly.\n",
    "\n",
    "Overall, this excercise has proven what has been discussed in the initial research report- while econometric analysis performed well or on par with more complex models, it was determined to be highly specific to the data it was built on. As spam detection is a fast evolving subject that would need frequent updating and flexibility, other models, such as machine learning, are preferable.\n",
    "\n",
    "## Disclaimer: AI Tools were used to reproduce the research\n",
    "\n",
    "Our translation from R to Python was aided by the use of ChatGPT. The process involved an initial machine translation, after which we have fixed mistakes in subsequent iterations. ChatGPT provided also an alternative to Stack Overflow while searching for solutions to bugs we encountered. While using ChatGPT we have used queries that contained R code and the error messages."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eff7ca13",
   "metadata": {},
   "source": [
    "Load the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf0eb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import patsy\n",
    "from statsmodels.discrete.discrete_model import ProbitResults, LogitResults\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import norm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1497ff39",
   "metadata": {},
   "source": [
    "Import the initial dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20412dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv(\"C:/Users/justy/Desktop/Info/Inne/DSC/UW/Semestr IV/RR/RR_Project/Initial research/spambase - raw data/spambase.data\", header=None)\n",
    "names = pd.read_csv(\"names.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2f3f7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         0     1     2    3     4     5     6     7     8     9   ...     48  \\\n",
       "0     0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "1     0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.000   \n",
       "2     0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.010   \n",
       "3     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "4     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "...    ...   ...   ...  ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "4596  0.31  0.00  0.62  0.0  0.00  0.31  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4597  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4598  0.30  0.00  0.30  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.102   \n",
       "4599  0.96  0.00  0.00  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4600  0.00  0.00  0.65  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "\n",
       "         49   50     51     52     53     54   55    56  57  \n",
       "0     0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1     0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2     0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3     0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4     0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "...     ...  ...    ...    ...    ...    ...  ...   ...  ..  \n",
       "4596  0.232  0.0  0.000  0.000  0.000  1.142    3    88   0  \n",
       "4597  0.000  0.0  0.353  0.000  0.000  1.555    4    14   0  \n",
       "4598  0.718  0.0  0.000  0.000  0.000  1.404    6   118   0  \n",
       "4599  0.057  0.0  0.000  0.000  0.000  1.147    5    78   0  \n",
       "4600  0.000  0.0  0.125  0.000  0.000  1.250    5    40   0  \n",
       "\n",
       "[4601 rows x 58 columns]>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "251713ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                              0\n",
       "0               word_freq_make\n",
       "1            word_freq_address\n",
       "2                word_freq_all\n",
       "3                 word_freq_3d\n",
       "4                word_freq_our\n",
       "5               word_freq_over\n",
       "6             word_freq_remove\n",
       "7           word_freq_internet\n",
       "8              word_freq_order\n",
       "9               word_freq_mail\n",
       "10           word_freq_receive\n",
       "11              word_freq_will\n",
       "12            word_freq_people\n",
       "13            word_freq_report\n",
       "14         word_freq_addresses\n",
       "15              word_freq_free\n",
       "16          word_freq_business\n",
       "17             word_freq_email\n",
       "18               word_freq_you\n",
       "19            word_freq_credit\n",
       "20              word_freq_your\n",
       "21              word_freq_font\n",
       "22               word_freq_000\n",
       "23             word_freq_money\n",
       "24                word_freq_hp\n",
       "25               word_freq_hpl\n",
       "26            word_freq_george\n",
       "27               word_freq_650\n",
       "28               word_freq_lab\n",
       "29              word_freq_labs\n",
       "30            word_freq_telnet\n",
       "31               word_freq_857\n",
       "32              word_freq_data\n",
       "33               word_freq_415\n",
       "34                word_freq_85\n",
       "35        word_freq_technology\n",
       "36              word_freq_1999\n",
       "37             word_freq_parts\n",
       "38                word_freq_pm\n",
       "39            word_freq_direct\n",
       "40                word_freq_cs\n",
       "41           word_freq_meeting\n",
       "42          word_freq_original\n",
       "43           word_freq_project\n",
       "44                word_freq_re\n",
       "45               word_freq_edu\n",
       "46             word_freq_table\n",
       "47        word_freq_conference\n",
       "48                 char_freq_;\n",
       "49                 char_freq_(\n",
       "50                 char_freq_[\n",
       "51                 char_freq_!\n",
       "52                 char_freq_$\n",
       "53                 char_freq_#\n",
       "54  capital_run_length_average\n",
       "55  capital_run_length_longest\n",
       "56    capital_run_length_total>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e25ea9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "\n",
    "names.loc[48, 0] = \"char_freq_semicolon\"\n",
    "names.loc[49, 0] = \"char_freq_bracket\"\n",
    "names.loc[50, 0] = \"char_freq_square_bracket\"\n",
    "names.loc[51, 0] = \"char_freq_exclamation\"\n",
    "names.loc[52, 0] = \"char_freq_dollar\"\n",
    "names.loc[53, 0] = \"char_freq_hashtag\"\n",
    "names.loc[57, 0] = \"spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e08fef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove missing values\n",
    "\n",
    "spam.columns = names[0].values\n",
    "\n",
    "spam.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7daf3969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d',\n",
       "       'word_freq_our', 'word_freq_over', 'word_freq_remove',\n",
       "       'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
       "       'word_freq_receive', 'word_freq_will', 'word_freq_people',\n",
       "       'word_freq_report', 'word_freq_addresses', 'word_freq_free',\n",
       "       'word_freq_business', 'word_freq_email', 'word_freq_you',\n",
       "       'word_freq_credit', 'word_freq_your', 'word_freq_font', 'word_freq_000',\n",
       "       'word_freq_money', 'word_freq_hp', 'word_freq_hpl', 'word_freq_george',\n",
       "       'word_freq_650', 'word_freq_lab', 'word_freq_labs', 'word_freq_telnet',\n",
       "       'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85',\n",
       "       'word_freq_technology', 'word_freq_1999', 'word_freq_parts',\n",
       "       'word_freq_pm', 'word_freq_direct', 'word_freq_cs', 'word_freq_meeting',\n",
       "       'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
       "       'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
       "       'char_freq_semicolon', 'char_freq_bracket', 'char_freq_square_bracket',\n",
       "       'char_freq_exclamation', 'char_freq_dollar', 'char_freq_hashtag',\n",
       "       'capital_run_length_average', 'capital_run_length_longest',\n",
       "       'capital_run_length_total', 'spam'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#overview of our database\n",
    "\n",
    "spam.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d595875b",
   "metadata": {},
   "source": [
    "Let us check the distribution of spam and non-spam mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea06ef2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spam_count = spam[\"spam\"].value_counts()\n",
    "perc_yes = round(spam_count[1] / len(spam) * 100, 2)\n",
    "perc_no = round(spam_count[0] / len(spam) * 100, 2)\n",
    "\n",
    "plt.bar([\"Non-spam\", \"Spam\"], spam_count, color=[\"#31d64f\", \"#ed3b3b\"])\n",
    "plt.title(\"Number of spam/non-spam mails\")\n",
    "plt.xlabel(\"Type of mail\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend([\"Non-spam\", \"Spam\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "207735ce",
   "metadata": {},
   "source": [
    "We see that the dataset consists of 1813 observations in the “spam” category and 2788 in the “non-spam” category, providing a ca. 40/60 class distribution. Moreover, there are no missing values in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4d25fc9",
   "metadata": {},
   "source": [
    "## Start from the most general model that contains all explanatory variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "451d0fa9",
   "metadata": {},
   "source": [
    "Create a formula that consists of all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b02c215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"spam ~ \" + \" + \".join(spam.columns[:-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "246073b9",
   "metadata": {},
   "source": [
    "We build two classification models, probit and logit, both binary dependent variable predictive models. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52ffe5b2",
   "metadata": {},
   "source": [
    "Probit inital model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0f4781df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.207573\n",
      "         Iterations 15\n",
      "                          Probit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                   spam   No. Observations:                 4601\n",
      "Model:                         Probit   Df Residuals:                     4543\n",
      "Method:                           MLE   Df Model:                           57\n",
      "Date:                Sun, 18 Jun 2023   Pseudo R-squ.:                  0.6904\n",
      "Time:                        22:37:48   Log-Likelihood:                -955.04\n",
      "converged:                       True   LL-Null:                       -3085.1\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================================\n",
      "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "Intercept                     -0.8356      0.075    -11.098      0.000      -0.983      -0.688\n",
      "word_freq_make                -0.1751      0.115     -1.520      0.128      -0.401       0.051\n",
      "word_freq_address             -0.0858      0.035     -2.462      0.014      -0.154      -0.017\n",
      "word_freq_all                  0.1134      0.063      1.803      0.071      -0.010       0.237\n",
      "word_freq_3d                   1.3766      0.803      1.713      0.087      -0.198       2.951\n",
      "word_freq_our                  0.3122      0.047      6.652      0.000       0.220       0.404\n",
      "word_freq_over                 0.4709      0.108      4.363      0.000       0.259       0.683\n",
      "word_freq_remove               0.9894      0.121      8.146      0.000       0.751       1.227\n",
      "word_freq_internet             0.2829      0.080      3.534      0.000       0.126       0.440\n",
      "word_freq_order                0.2988      0.120      2.493      0.013       0.064       0.534\n",
      "word_freq_mail                 0.0786      0.039      2.033      0.042       0.003       0.154\n",
      "word_freq_receive             -0.0800      0.151     -0.531      0.595      -0.375       0.215\n",
      "word_freq_will                -0.0867      0.041     -2.129      0.033      -0.167      -0.007\n",
      "word_freq_people              -0.0279      0.126     -0.221      0.825      -0.276       0.220\n",
      "word_freq_report               0.1086      0.078      1.396      0.163      -0.044       0.261\n",
      "word_freq_addresses            0.8329      0.362      2.298      0.022       0.122       1.543\n",
      "word_freq_free                 0.5191      0.062      8.437      0.000       0.399       0.640\n",
      "word_freq_business             0.4645      0.109      4.248      0.000       0.250       0.679\n",
      "word_freq_email                0.1029      0.064      1.595      0.111      -0.024       0.229\n",
      "word_freq_you                  0.0401      0.019      2.079      0.038       0.002       0.078\n",
      "word_freq_credit               0.4384      0.211      2.079      0.038       0.025       0.852\n",
      "word_freq_your                 0.1487      0.028      5.359      0.000       0.094       0.203\n",
      "word_freq_font                 0.1434      0.087      1.652      0.098      -0.027       0.313\n",
      "word_freq_000                  1.2991      0.212      6.142      0.000       0.885       1.714\n",
      "word_freq_money                0.2165      0.062      3.466      0.001       0.094       0.339\n",
      "word_freq_hp                  -0.7846      0.108     -7.271      0.000      -0.996      -0.573\n",
      "word_freq_hpl                 -0.6996      0.200     -3.505      0.000      -1.091      -0.308\n",
      "word_freq_george              -3.5943      0.564     -6.370      0.000      -4.700      -2.488\n",
      "word_freq_650                  0.2448      0.101      2.421      0.015       0.047       0.443\n",
      "word_freq_lab                 -1.4529      0.775     -1.874      0.061      -2.972       0.067\n",
      "word_freq_labs                -0.2275      0.166     -1.375      0.169      -0.552       0.097\n",
      "word_freq_telnet              -0.0992      0.231     -0.429      0.668      -0.553       0.354\n",
      "word_freq_857                  1.1786      1.536      0.767      0.443      -1.832       4.189\n",
      "word_freq_data                -0.4158      0.161     -2.590      0.010      -0.730      -0.101\n",
      "word_freq_415                 -0.2320      0.819     -0.283      0.777      -1.837       1.373\n",
      "word_freq_85                  -1.2428      0.406     -3.060      0.002      -2.039      -0.447\n",
      "word_freq_technology           0.4606      0.161      2.860      0.004       0.145       0.776\n",
      "word_freq_1999                 0.0014      0.097      0.014      0.989      -0.188       0.191\n",
      "word_freq_parts               -0.3007      0.227     -1.324      0.185      -0.746       0.144\n",
      "word_freq_pm                  -0.4698      0.208     -2.255      0.024      -0.878      -0.061\n",
      "word_freq_direct              -0.1422      0.201     -0.708      0.479      -0.536       0.251\n",
      "word_freq_cs                 -24.2110     13.248     -1.828      0.068     -50.177       1.755\n",
      "word_freq_meeting             -1.5055      0.431     -3.496      0.000      -2.350      -0.662\n",
      "word_freq_original            -0.6552      0.400     -1.640      0.101      -1.438       0.128\n",
      "word_freq_project             -0.8125      0.264     -3.081      0.002      -1.329      -0.296\n",
      "word_freq_re                  -0.4094      0.074     -5.507      0.000      -0.555      -0.264\n",
      "word_freq_edu                 -0.6755      0.114     -5.923      0.000      -0.899      -0.452\n",
      "word_freq_table               -1.2250      0.764     -1.604      0.109      -2.722       0.272\n",
      "word_freq_conference          -2.0125      0.746     -2.699      0.007      -3.474      -0.551\n",
      "char_freq_semicolon           -0.8079      0.248     -3.257      0.001      -1.294      -0.322\n",
      "char_freq_bracket             -0.1302      0.139     -0.933      0.351      -0.404       0.143\n",
      "char_freq_square_bracket      -0.4066      0.452     -0.899      0.369      -1.293       0.480\n",
      "char_freq_exclamation          0.1572      0.027      5.901      0.000       0.105       0.209\n",
      "char_freq_dollar               2.2585      0.283      7.994      0.000       1.705       2.812\n",
      "char_freq_hashtag              1.3189      0.329      4.012      0.000       0.675       1.963\n",
      "capital_run_length_average    -0.0027      0.008     -0.333      0.739      -0.019       0.013\n",
      "capital_run_length_longest     0.0043      0.001      3.595      0.000       0.002       0.007\n",
      "capital_run_length_total       0.0005   9.83e-05      5.006      0.000       0.000       0.001\n",
      "==============================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.33 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "myprobit = sm.Probit.from_formula(formula, data=spam).fit()\n",
    "print(myprobit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5a94060",
   "metadata": {},
   "source": [
    "Logit initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69db3ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197323\n",
      "         Iterations 15\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                   spam   No. Observations:                 4601\n",
      "Model:                          Logit   Df Residuals:                     4543\n",
      "Method:                           MLE   Df Model:                           57\n",
      "Date:                Sun, 18 Jun 2023   Pseudo R-squ.:                  0.7057\n",
      "Time:                        22:37:49   Log-Likelihood:                -907.88\n",
      "converged:                       True   LL-Null:                       -3085.1\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================================\n",
      "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "Intercept                     -1.5686      0.142    -11.044      0.000      -1.847      -1.290\n",
      "word_freq_make                -0.3895      0.231     -1.683      0.092      -0.843       0.064\n",
      "word_freq_address             -0.1458      0.069     -2.104      0.035      -0.282      -0.010\n",
      "word_freq_all                  0.1141      0.110      1.035      0.301      -0.102       0.330\n",
      "word_freq_3d                   2.2515      1.507      1.494      0.135      -0.702       5.205\n",
      "word_freq_our                  0.5624      0.102      5.524      0.000       0.363       0.762\n",
      "word_freq_over                 0.8830      0.250      3.534      0.000       0.393       1.373\n",
      "word_freq_remove               2.2785      0.333      6.846      0.000       1.626       2.931\n",
      "word_freq_internet             0.5696      0.168      3.387      0.001       0.240       0.899\n",
      "word_freq_order                0.7343      0.285      2.577      0.010       0.176       1.293\n",
      "word_freq_mail                 0.1275      0.073      1.755      0.079      -0.015       0.270\n",
      "word_freq_receive             -0.2557      0.298     -0.858      0.391      -0.840       0.328\n",
      "word_freq_will                -0.1383      0.074     -1.868      0.062      -0.283       0.007\n",
      "word_freq_people              -0.0796      0.230     -0.346      0.730      -0.531       0.372\n",
      "word_freq_report               0.1447      0.136      1.061      0.289      -0.123       0.412\n",
      "word_freq_addresses            1.2362      0.725      1.704      0.088      -0.186       2.658\n",
      "word_freq_free                 1.0386      0.146      7.128      0.000       0.753       1.324\n",
      "word_freq_business             0.9599      0.225      4.264      0.000       0.519       1.401\n",
      "word_freq_email                0.1203      0.117      1.027      0.305      -0.109       0.350\n",
      "word_freq_you                  0.0813      0.035      2.320      0.020       0.013       0.150\n",
      "word_freq_credit               1.0474      0.538      1.946      0.052      -0.008       2.102\n",
      "word_freq_your                 0.2419      0.052      4.615      0.000       0.139       0.345\n",
      "word_freq_font                 0.2013      0.163      1.238      0.216      -0.117       0.520\n",
      "word_freq_000                  2.2452      0.471      4.762      0.000       1.321       3.169\n",
      "word_freq_money                0.4264      0.162      2.630      0.009       0.109       0.744\n",
      "word_freq_hp                  -1.9204      0.313     -6.139      0.000      -2.534      -1.307\n",
      "word_freq_hpl                 -1.0402      0.440     -2.366      0.018      -1.902      -0.179\n",
      "word_freq_george             -11.7672      2.113     -5.569      0.000     -15.909      -7.626\n",
      "word_freq_650                  0.4454      0.199      2.237      0.025       0.055       0.836\n",
      "word_freq_lab                 -2.4864      1.502     -1.656      0.098      -5.429       0.457\n",
      "word_freq_labs                -0.3299      0.314     -1.052      0.293      -0.945       0.285\n",
      "word_freq_telnet              -0.1702      0.482     -0.353      0.724      -1.114       0.774\n",
      "word_freq_857                  2.5488      3.283      0.776      0.438      -3.886       8.984\n",
      "word_freq_data                -0.7383      0.312     -2.369      0.018      -1.349      -0.127\n",
      "word_freq_415                  0.6679      1.601      0.417      0.676      -2.469       3.805\n",
      "word_freq_85                  -2.0554      0.788     -2.607      0.009      -3.601      -0.510\n",
      "word_freq_technology           0.9237      0.309      2.989      0.003       0.318       1.530\n",
      "word_freq_1999                 0.0465      0.175      0.265      0.791      -0.297       0.390\n",
      "word_freq_parts               -0.5968      0.423     -1.410      0.158      -1.426       0.233\n",
      "word_freq_pm                  -0.8650      0.383     -2.260      0.024      -1.615      -0.115\n",
      "word_freq_direct              -0.3046      0.364     -0.838      0.402      -1.017       0.408\n",
      "word_freq_cs                 -45.0480     26.600     -1.694      0.090     -97.182       7.086\n",
      "word_freq_meeting             -2.6887      0.838     -3.207      0.001      -4.332      -1.045\n",
      "word_freq_original            -1.2471      0.806     -1.547      0.122      -2.828       0.333\n",
      "word_freq_project             -1.5732      0.529     -2.973      0.003      -2.610      -0.536\n",
      "word_freq_re                  -0.7923      0.156     -5.091      0.000      -1.097      -0.487\n",
      "word_freq_edu                 -1.4592      0.269     -5.434      0.000      -1.986      -0.933\n",
      "word_freq_table               -2.3259      1.659     -1.402      0.161      -5.578       0.926\n",
      "word_freq_conference          -4.0156      1.611     -2.493      0.013      -7.173      -0.858\n",
      "char_freq_semicolon           -1.2911      0.442     -2.920      0.004      -2.158      -0.424\n",
      "char_freq_bracket             -0.1881      0.249     -0.754      0.451      -0.677       0.301\n",
      "char_freq_square_bracket      -0.6574      0.838     -0.784      0.433      -2.301       0.986\n",
      "char_freq_exclamation          0.3472      0.089      3.890      0.000       0.172       0.522\n",
      "char_freq_dollar               5.3360      0.706      7.553      0.000       3.951       6.721\n",
      "char_freq_hashtag              2.4032      1.113      2.159      0.031       0.221       4.585\n",
      "capital_run_length_average     0.0120      0.019      0.636      0.525      -0.025       0.049\n",
      "capital_run_length_longest     0.0091      0.003      3.618      0.000       0.004       0.014\n",
      "capital_run_length_total       0.0008      0.000      3.747      0.000       0.000       0.001\n",
      "==============================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.28 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "mylogit = sm.Logit.from_formula(formula, data=spam).fit()\n",
    "print(mylogit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "433aef44",
   "metadata": {},
   "source": [
    "# Significance test of models coeeficients - LR test\n",
    "\n",
    "H0: Beta1 = 0 & Beta2 = 0 & … & Beta57 = 0 (the general model can be simplified to restricted model based only on constant / all Betas are jointly insignificant)\n",
    "H1: The general model cannot be simplified to a restricted model. All Betas are jointly significant.\n",
    "\n",
    "Both models p-values are 0, so null hypothesis can be rejected. It means that for both models, the coefficients are jointly significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3af6d927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.670523\n",
      "         Iterations 4\n",
      "Probit likelihood ratio test p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "null_probit = sm.Probit(spam[\"spam\"], sm.add_constant(pd.Series([1] * len(spam)))).fit()\n",
    "probit_lrtest = stats.chi2.sf(2 * (myprobit.llf - null_probit.llf), 1)\n",
    "print(\"Probit likelihood ratio test p-value:\", probit_lrtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb8447e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.670523\n",
      "         Iterations 4\n",
      "Logit likelihood ratio test p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "null_logit = sm.Logit(spam[\"spam\"], sm.add_constant(pd.Series([1] * len(spam)))).fit()\n",
    "logit_lrtest = stats.chi2.sf(2 * (mylogit.llf - null_logit.llf), 1)\n",
    "print(\"Logit likelihood ratio test p-value:\", logit_lrtest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68ab87c0",
   "metadata": {},
   "source": [
    "## Stepwise regression\n",
    "\n",
    "As not all variables are individually significant, we perform the stepwise regression, which removes variables below the 5% significance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f50e6e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_1999\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.207573\n",
      "         Iterations 15\n",
      "2024.088917331387\n",
      "word_freq_people\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.207579\n",
      "         Iterations 15\n",
      "2022.1381806395907\n",
      "word_freq_415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.207587\n",
      "         Iterations 15\n",
      "2020.2155613834486\n",
      "capital_run_length_average\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.207597\n",
      "         Iterations 15\n",
      "2018.3090124810756\n",
      "word_freq_telnet\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.207632\n",
      "         Iterations 15\n",
      "2016.6278326615745\n",
      "word_freq_receive\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.207662\n",
      "         Iterations 15\n",
      "2014.9078328699995\n",
      "word_freq_direct\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.207723\n",
      "         Iterations 15\n",
      "2013.4677960927206\n",
      "word_freq_857\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.207781\n",
      "         Iterations 15\n",
      "2011.9973433071923\n",
      "char_freq_bracket\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.207870\n",
      "         Iterations 15\n",
      "2010.8215432666034\n",
      "char_freq_square_bracket\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.208013\n",
      "         Iterations 15\n",
      "2010.1315739685158\n",
      "word_freq_parts\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.208293\n",
      "         Iterations 15\n",
      "2010.7134011600897\n",
      "word_freq_labs\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.208496\n",
      "         Iterations 15\n",
      "2010.5829619277797\n",
      "word_freq_report\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.208711\n",
      "         Iterations 15\n",
      "2010.5603090443033\n",
      "word_freq_table\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.209245\n",
      "         Iterations 15\n",
      "2013.470853221295\n",
      "word_freq_email\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.209514\n",
      "         Iterations 15\n",
      "2013.9508016037705\n",
      "word_freq_original\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.209999\n",
      "         Iterations 15\n",
      "2016.412114345779\n",
      "word_freq_3d\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.211320\n",
      "         Iterations 15\n",
      "2026.567296884151\n",
      "word_freq_font\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.211849\n",
      "         Iterations 15\n",
      "2029.4377589667429\n",
      "word_freq_all\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.212171\n",
      "         Iterations 15\n",
      "2030.398643225174\n",
      "word_freq_mail\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.212520\n",
      "         Iterations 15\n",
      "2031.6074027190002\n",
      "word_freq_cs\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.215982\n",
      "         Iterations 13\n",
      "2061.4663901491\n",
      "word_freq_lab\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.217255\n",
      "         Iterations 13\n",
      "2071.1786079184076\n",
      "word_freq_make\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.217638\n",
      "         Iterations 13\n",
      "2072.7084984515222\n"
     ]
    }
   ],
   "source": [
    "p_probit = myprobit.pvalues\n",
    "spam_temp_probit = spam.copy()\n",
    "\n",
    "while any(p_probit > 0.05):\n",
    "    worstp = p_probit.idxmax()\n",
    "    \n",
    "    print(worstp)\n",
    "    spam_temp_probit.drop(columns=worstp, inplace=True)\n",
    "    \n",
    "    formula = \"spam ~\"\n",
    "    \n",
    "    for column in spam_temp_probit.columns[:-1]:\n",
    "        formula += f\" + {column}\"\n",
    "    \n",
    "    myprobit = sm.Probit.from_formula(formula, data=spam_temp_probit).fit()\n",
    "    p_probit = myprobit.pvalues\n",
    "    print(myprobit.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "09ce8b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_1999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197330\n",
      "         Iterations 15\n",
      "1929.8351001472463\n",
      "word_freq_telnet\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197358\n",
      "         Iterations 15\n",
      "1928.0866448894978\n",
      "word_freq_people\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197372\n",
      "         Iterations 15\n",
      "1926.2137629946117\n",
      "word_freq_415\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197392\n",
      "         Iterations 15\n",
      "1924.4056155735882\n",
      "capital_run_length_average\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197440\n",
      "         Iterations 15\n",
      "1922.846172866969\n",
      "char_freq_bracket\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197500\n",
      "         Iterations 15\n",
      "1921.3991742674934\n",
      "word_freq_857\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197573\n",
      "         Iterations 15\n",
      "1920.0647903082443\n",
      "char_freq_square_bracket\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197678\n",
      "         Iterations 15\n",
      "1919.0337948154875\n",
      "word_freq_direct\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197763\n",
      "         Iterations 15\n",
      "1917.8151918506705\n",
      "word_freq_receive\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197849\n",
      "         Iterations 15\n",
      "1916.6081205165324\n",
      "word_freq_email\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197952\n",
      "         Iterations 15\n",
      "1915.5504717918839\n",
      "word_freq_report\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.198051\n",
      "         Iterations 15\n",
      "1914.4675229356426\n",
      "word_freq_labs\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.198186\n",
      "         Iterations 15\n",
      "1913.7108461065413\n",
      "word_freq_all\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.198313\n",
      "         Iterations 15\n",
      "1912.8758937291022\n",
      "word_freq_table\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.198761\n",
      "         Iterations 15\n",
      "1914.9965439222237\n",
      "word_freq_font\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.199029\n",
      "         Iterations 15\n",
      "1915.4664193981243\n",
      "word_freq_parts\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.199435\n",
      "         Iterations 15\n",
      "1917.1989978378258\n",
      "word_freq_3d\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.200416\n",
      "         Iterations 15\n",
      "1924.2281989342282\n",
      "word_freq_original\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.200923\n",
      "         Iterations 15\n",
      "1926.890731198925\n",
      "word_freq_lab\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.201978\n",
      "         Iterations 15\n",
      "1934.5974196686414\n",
      "word_freq_mail\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.202278\n",
      "         Iterations 15\n",
      "1935.3616724448575\n",
      "word_freq_cs\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.205096\n",
      "         Iterations 13\n",
      "1959.2969039457766\n",
      "word_freq_will\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.205470\n",
      "         Iterations 13\n",
      "1960.7358706869513\n",
      "word_freq_addresses\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.205945\n",
      "         Iterations 13\n",
      "1963.1088921606406\n"
     ]
    }
   ],
   "source": [
    "p_logit = mylogit.pvalues\n",
    "spam_temp_logit = spam.copy()\n",
    "\n",
    "while any(p_logit > 0.05):\n",
    "    worstp = p_logit.idxmax()\n",
    "    \n",
    "    print(worstp)\n",
    "    spam_temp_logit.drop(columns=worstp, inplace=True)\n",
    "    \n",
    "    formula = \"spam ~\"\n",
    "    \n",
    "    for column in spam_temp_logit.columns[:-1]:\n",
    "        formula += f\" + {column}\"\n",
    "    \n",
    "    mylogit = sm.Logit.from_formula(formula, data=spam_temp_logit).fit()\n",
    "    p_logit = mylogit.pvalues\n",
    "    print(mylogit.aic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "415ba833",
   "metadata": {},
   "source": [
    "## Link test - it verifies the model specification\n",
    "\n",
    "If yhat is significant and yhat2 is not signifficant: model has a good specification\n",
    "\n",
    "If yhat is / is not significant and yhat2 is signifficant: model does not have a good specification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b316024b",
   "metadata": {},
   "source": [
    "The defined functions were recreated from R codes that can be found in the \"Initial Research\" folder on GitHub. The functions were written by dr Rafal Wozniak, Faculty of Economic Sciences, University of Warsaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a7e8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linktest_probit(model):\n",
    "    \"\"\"\n",
    "    Function to perform linktest on a logistic regression model.\n",
    "    \n",
    "    Args:\n",
    "    - model: logistic regression model object\n",
    "    \n",
    "    Returns:\n",
    "    - aux_reg: auxiliary regression model object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the data\n",
    "    y = model.model.endog\n",
    "    pred = model.predict()\n",
    "    pred = np.clip(pred, 1e-12, 1 - 1e-12)\n",
    "    yhat = np.log(pred/(1-pred))\n",
    "    yhat2 = yhat**2\n",
    "\n",
    "    # Add constant column to predictor variables\n",
    "    X = np.column_stack((np.ones_like(y), yhat, yhat2))\n",
    "\n",
    "    # Remove rows with missing or infinite values\n",
    "    valid_idx = np.isfinite(X).all(axis=1)\n",
    "    X = X[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "\n",
    "    # Fit the binomial regression model\n",
    "    model = sm.GLM(y, X, family=sm.families.Binomial(link=sm.genmod.families.links.probit()))\n",
    "    result = model.fit()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0e1be571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linktest_logit(model):\n",
    "    \"\"\"\n",
    "    Function to perform linktest on a logistic regression model.\n",
    "    \n",
    "    Args:\n",
    "    - model: logistic regression model object\n",
    "    \n",
    "    Returns:\n",
    "    - aux_reg: auxiliary regression model object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the data\n",
    "    y = model.model.endog\n",
    "    pred = model.predict()\n",
    "    pred = np.clip(pred, 1e-12, 1 - 1e-12)\n",
    "    yhat = np.log(pred/(1-pred))\n",
    "    yhat2 = yhat**2\n",
    "\n",
    "    # Add constant column to predictor variables\n",
    "    X = np.column_stack((np.ones_like(y), yhat, yhat2))\n",
    "\n",
    "    # Remove rows with missing or infinite values\n",
    "    valid_idx = np.isfinite(X).all(axis=1)\n",
    "    X = X[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "\n",
    "    # Fit the binomial regression model\n",
    "    model = sm.GLM(y, X, family=sm.families.Binomial(link=sm.genmod.families.links.logit()))\n",
    "    result = model.fit()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aad47d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 4601\n",
      "Model:                            GLM   Df Residuals:                     4598\n",
      "Model Family:                Binomial   Df Model:                            2\n",
      "Link Function:                 probit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                    nan\n",
      "Date:                Sun, 18 Jun 2023   Deviance:                       34907.\n",
      "Time:                        22:38:17   Pearson chi2:                 1.71e+18\n",
      "No. Iterations:                   100   Pseudo R-squ. (CS):                nan\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const      -4.972e+12    1.4e+05  -3.54e+07      0.000   -4.97e+12   -4.97e+12\n",
      "x1          2.363e+13   1.21e+04   1.95e+09      0.000    2.36e+13    2.36e+13\n",
      "x2          2.199e+11    536.177    4.1e+08      0.000     2.2e+11     2.2e+11\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justy\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1014: RuntimeWarning: divide by zero encountered in log\n",
      "  special.gammaln(n - y + 1) + y * np.log(mu / (1 - mu + 1e-20)) +\n",
      "c:\\Users\\justy\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1014: RuntimeWarning: invalid value encountered in multiply\n",
      "  special.gammaln(n - y + 1) + y * np.log(mu / (1 - mu + 1e-20)) +\n"
     ]
    }
   ],
   "source": [
    "# Linktest for probit model - after stepwise regression\n",
    "linktest_result_probit = linktest_probit(myprobit)\n",
    "print(linktest_result_probit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cdcefa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 4601\n",
      "Model:                            GLM   Df Residuals:                     4598\n",
      "Model Family:                Binomial   Df Model:                            2\n",
      "Link Function:                  logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -940.83\n",
      "Date:                Sun, 18 Jun 2023   Deviance:                       1881.7\n",
      "Time:                        22:38:17   Pearson chi2:                 7.32e+09\n",
      "No. Iterations:                    16   Pseudo R-squ. (CS):             0.6063\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0902      0.060      1.501      0.133      -0.028       0.208\n",
      "x1             1.0204      0.036     28.024      0.000       0.949       1.092\n",
      "x2            -0.0319      0.004     -8.966      0.000      -0.039      -0.025\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Linktest for logit model - after stepwise regression\n",
    "linktest_result_logit = linktest_logit(mylogit)\n",
    "print(linktest_result_logit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42a2b95a",
   "metadata": {},
   "source": [
    "Based on the previousle mentioned criteria both model does not have good specification. The results are the same as in R.\n",
    "\n",
    "The next thing to consider is adding interaction terms to the models, so that it may help with the correct specification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "530c1348",
   "metadata": {},
   "source": [
    "### Interaction terms\n",
    "Adding interaction terms and deleting insignificant ones for probit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d94c22fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept                                                                       2.345811e-40\n",
      "word_freq_make                                                                  3.224714e-03\n",
      "word_freq_address                                                               4.307141e-02\n",
      "word_freq_make:word_freq_address                                                8.881067e-01\n",
      "word_freq_our                                                                   2.474520e-06\n",
      "                                                                                    ...     \n",
      "char_freq_semicolon:char_freq_dollar:char_freq_hashtag                          5.911354e-01\n",
      "char_freq_exclamation:char_freq_dollar:char_freq_hashtag                        8.673990e-01\n",
      "char_freq_semicolon:char_freq_exclamation:char_freq_dollar:char_freq_hashtag    4.725009e-01\n",
      "capital_run_length_longest                                                      1.011262e-11\n",
      "capital_run_length_total                                                        2.849669e-03\n",
      "Length: 61, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "formula_interactions = \"spam ~ word_freq_make * word_freq_address * word_freq_our + word_freq_over + word_freq_remove + word_freq_internet * word_freq_order * word_freq_free * word_freq_business + word_freq_you + word_freq_credit * word_freq_your + word_freq_000 + word_freq_money + word_freq_hp + word_freq_hpl + word_freq_george + word_freq_650 + word_freq_data + word_freq_85 + word_freq_technology + word_freq_pm + word_freq_meeting + word_freq_project + word_freq_re + word_freq_edu + word_freq_conference + char_freq_semicolon * char_freq_exclamation * char_freq_dollar * char_freq_hashtag + capital_run_length_longest + capital_run_length_total\"\n",
    "\n",
    "mylogit = sm.formula.glm(formula_interactions, data=spam, family=sm.families.Binomial(sm.genmod.families.links.logit())).fit()\n",
    "\n",
    "p = mylogit.pvalues\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a25a7e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_make:word_freq_address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_make:word_freq_address\n",
      "1920.299552620765\n",
      "word_freq_free:word_freq_business\n",
      "1918.3214384510645\n",
      "char_freq_exclamation:char_freq_dollar:char_freq_hashtag\n",
      "1916.348191039537\n",
      "char_freq_dollar:char_freq_hashtag\n",
      "1914.4148815341832\n",
      "word_freq_internet:word_freq_order\n",
      "1912.4856492302165\n",
      "char_freq_semicolon:char_freq_dollar\n",
      "1910.5750425935366\n",
      "char_freq_semicolon\n",
      "1908.6438618622913\n",
      "word_freq_internet:word_freq_order:word_freq_free\n",
      "1907.0447361830134\n",
      "word_freq_order:word_freq_free:word_freq_business\n",
      "1905.190842218639\n",
      "char_freq_semicolon:char_freq_exclamation:char_freq_hashtag\n",
      "1903.310937284863\n",
      "word_freq_internet:word_freq_order:word_freq_free:word_freq_business\n",
      "1902.5509796521967\n",
      "char_freq_semicolon:char_freq_dollar:char_freq_hashtag\n",
      "1904.0768697569106\n",
      "word_freq_internet:word_freq_free:word_freq_business\n",
      "1902.9043406399896\n",
      "word_freq_make:word_freq_address:word_freq_our\n",
      "1901.8527243664184\n",
      "word_freq_internet:word_freq_order:word_freq_business\n",
      "1902.6999170599224\n",
      "word_freq_internet:word_freq_business\n",
      "1900.9356701267802\n",
      "char_freq_semicolon:char_freq_exclamation:char_freq_dollar\n",
      "1901.7231435100807\n",
      "char_freq_semicolon:char_freq_exclamation:char_freq_dollar:char_freq_hashtag\n",
      "1899.7247495339152\n",
      "char_freq_exclamation:char_freq_hashtag\n",
      "1903.578313488078\n",
      "word_freq_make:word_freq_our\n",
      "1904.9415055038694\n",
      "word_freq_internet:word_freq_free\n",
      "1907.0817098338614\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while any(p > 0.05):\n",
    "    worstp = p.idxmax()\n",
    "    print(worstp)\n",
    "\n",
    "    if i == 1:\n",
    "        # Remove the outcome variable from the formula\n",
    "        formula_interactions = formula_interactions.replace(\"spam ~ \", \"\")\n",
    "\n",
    "        # Create the design matrix with interaction terms\n",
    "        X = patsy.dmatrix(formula_interactions, data=spam)\n",
    "\n",
    "        # Convert the design matrix to a DataFrame\n",
    "        X = pd.DataFrame(X, columns=X.design_info.column_names)\n",
    "        i=2\n",
    "    else:\n",
    "        X = X.drop(worstp, axis=1)\n",
    "        X_names = ['Intercept'] + list(X.columns)[1:]\n",
    "        X.columns = X_names\n",
    "\n",
    "        mylogit = sm.GLM(spam['spam'], X, family=sm.families.Binomial(sm.families.links.logit())).fit()\n",
    "\n",
    "        p = mylogit.pvalues\n",
    "        print(mylogit.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c174e69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                   spam   No. Observations:                 4601\n",
      "Model:                            GLM   Df Residuals:                     4561\n",
      "Model Family:                Binomial   Df Model:                           39\n",
      "Link Function:                  logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -913.54\n",
      "Date:                Sun, 18 Jun 2023   Deviance:                       1827.1\n",
      "Time:                        22:38:29   Pearson chi2:                 3.35e+06\n",
      "No. Iterations:                    12   Pseudo R-squ. (CS):             0.6109\n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================================\n",
      "                                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                    -1.6595      0.121    -13.711      0.000      -1.897      -1.422\n",
      "word_freq_make                               -0.5586      0.222     -2.516      0.012      -0.994      -0.124\n",
      "word_freq_address                            -0.1505      0.075     -2.009      0.045      -0.297      -0.004\n",
      "word_freq_our                                 0.5501      0.103      5.347      0.000       0.348       0.752\n",
      "word_freq_address:word_freq_our               1.1544      0.462      2.497      0.013       0.248       2.060\n",
      "word_freq_over                                0.7901      0.248      3.189      0.001       0.305       1.276\n",
      "word_freq_remove                              2.4200      0.338      7.163      0.000       1.758       3.082\n",
      "word_freq_internet                            0.5753      0.158      3.645      0.000       0.266       0.885\n",
      "word_freq_order                               1.3900      0.318      4.378      0.000       0.768       2.012\n",
      "word_freq_free                                1.0134      0.143      7.073      0.000       0.733       1.294\n",
      "word_freq_order:word_freq_free               -1.3862      0.690     -2.008      0.045      -2.739      -0.033\n",
      "word_freq_business                            1.1386      0.231      4.927      0.000       0.686       1.592\n",
      "word_freq_order:word_freq_business           -0.8560      0.173     -4.937      0.000      -1.196      -0.516\n",
      "word_freq_you                                 0.0876      0.034      2.552      0.011       0.020       0.155\n",
      "word_freq_credit                              1.8744      0.643      2.914      0.004       0.614       3.135\n",
      "word_freq_your                                0.2142      0.048      4.422      0.000       0.119       0.309\n",
      "word_freq_credit:word_freq_your              -0.4809      0.230     -2.093      0.036      -0.931      -0.031\n",
      "word_freq_000                                 2.2078      0.445      4.962      0.000       1.336       3.080\n",
      "word_freq_money                               0.5605      0.191      2.938      0.003       0.187       0.934\n",
      "word_freq_hp                                 -2.1414      0.288     -7.444      0.000      -2.705      -1.578\n",
      "word_freq_hpl                                -1.4390      0.469     -3.067      0.002      -2.359      -0.520\n",
      "word_freq_george                            -12.7754      1.777     -7.189      0.000     -16.259      -9.292\n",
      "word_freq_650                                 0.5676      0.253      2.239      0.025       0.071       1.064\n",
      "word_freq_data                               -0.8550      0.317     -2.698      0.007      -1.476      -0.234\n",
      "word_freq_85                                 -2.5413      1.074     -2.365      0.018      -4.647      -0.435\n",
      "word_freq_technology                          0.9503      0.321      2.962      0.003       0.322       1.579\n",
      "word_freq_pm                                 -1.2787      0.423     -3.024      0.002      -2.108      -0.450\n",
      "word_freq_meeting                            -3.1280      0.894     -3.499      0.000      -4.880      -1.376\n",
      "word_freq_project                            -2.0414      0.576     -3.544      0.000      -3.170      -0.912\n",
      "word_freq_re                                 -0.7778      0.152     -5.115      0.000      -1.076      -0.480\n",
      "word_freq_edu                                -1.6534      0.285     -5.793      0.000      -2.213      -1.094\n",
      "word_freq_conference                         -4.5504      1.702     -2.674      0.008      -7.886      -1.215\n",
      "char_freq_exclamation                         0.2852      0.074      3.830      0.000       0.139       0.431\n",
      "char_freq_semicolon:char_freq_exclamation     3.7388      1.219      3.066      0.002       1.349       6.129\n",
      "char_freq_dollar                              3.0106      0.774      3.890      0.000       1.494       4.527\n",
      "char_freq_exclamation:char_freq_dollar       14.8007      3.707      3.993      0.000       7.535      22.066\n",
      "char_freq_hashtag                             3.5518      0.596      5.961      0.000       2.384       4.720\n",
      "char_freq_semicolon:char_freq_hashtag        -4.7141      1.991     -2.367      0.018      -8.617      -0.811\n",
      "capital_run_length_longest                    0.0124      0.002      7.078      0.000       0.009       0.016\n",
      "capital_run_length_total                      0.0005      0.000      3.136      0.002       0.000       0.001\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(mylogit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cf39f77",
   "metadata": {},
   "source": [
    "The used varaibles in the model are exactly the same as in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a890f6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.670523\n",
      "         Iterations 4\n",
      "Logit likelihood ratio test p-value: 0.0\n",
      "LinkTest result                  Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 4601\n",
      "Model:                            GLM   Df Residuals:                     4598\n",
      "Model Family:                Binomial   Df Model:                            2\n",
      "Link Function:                  logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -913.31\n",
      "Date:                Sun, 18 Jun 2023   Deviance:                       1826.6\n",
      "Time:                        22:38:29   Pearson chi2:                 2.82e+07\n",
      "No. Iterations:                    11   Pseudo R-squ. (CS):             0.6109\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0254      0.071      0.360      0.719      -0.113       0.164\n",
      "x1             1.0005      0.037     27.086      0.000       0.928       1.073\n",
      "x2            -0.0096      0.014     -0.688      0.492      -0.037       0.018\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "null_logit = sm.Logit(spam[\"spam\"], sm.add_constant(pd.Series([1] * len(spam)))).fit()\n",
    "logit_lrtest = stats.chi2.sf(2 * (mylogit.llf - null_logit.llf), 1)\n",
    "print(\"Logit likelihood ratio test p-value:\", logit_lrtest)\n",
    "\n",
    "linktest_result_logit = linktest_logit(mylogit)\n",
    "print(\"LinkTest result\", linktest_result_logit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "558da44f",
   "metadata": {},
   "source": [
    "It seems that yhat is significant and yhat squared is not - this means that now updated model with interactions is correctly specified. The same results were obtained in the initial research."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc2ecfb3",
   "metadata": {},
   "source": [
    "Now let us test the probit model with interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "add39fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.268006\n",
      "         Iterations 13\n",
      "Intercept                                           1.060829e-58\n",
      "word_freq_free                                      2.013689e-27\n",
      "word_freq_order                                     3.707155e-05\n",
      "word_freq_receive                                   5.922157e-01\n",
      "char_freq_hashtag                                   4.317635e-02\n",
      "char_freq_exclamation                               6.030368e-08\n",
      "char_freq_hashtag:char_freq_exclamation             9.817137e-05\n",
      "word_freq_telnet                                    1.765068e-01\n",
      "word_freq_technology                                1.495378e-03\n",
      "word_freq_conference                                4.159086e-03\n",
      "word_freq_edu                                       1.100515e-15\n",
      "word_freq_conference:word_freq_edu                  5.262433e-01\n",
      "word_freq_hp                                        8.853227e-28\n",
      "word_freq_money                                     7.248825e-08\n",
      "word_freq_credit                                    4.063458e-06\n",
      "word_freq_000                                       1.663308e-16\n",
      "char_freq_dollar                                    4.524471e-28\n",
      "word_freq_your                                      1.075997e-10\n",
      "word_freq_email                                     7.861416e-04\n",
      "word_freq_your:word_freq_email                      8.424188e-02\n",
      "word_freq_address                                   1.838695e-02\n",
      "word_freq_your:word_freq_address                    6.701269e-03\n",
      "word_freq_email:word_freq_address                   1.632713e-03\n",
      "word_freq_your:word_freq_email:word_freq_address    1.340764e-04\n",
      "word_freq_people                                    1.774247e-01\n",
      "word_freq_mail                                      6.863608e-03\n",
      "word_freq_george                                    8.802906e-10\n",
      "word_freq_our                                       5.436798e-18\n",
      "word_freq_meeting                                   1.220579e-03\n",
      "word_freq_our:word_freq_meeting                     7.867079e-01\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "formula_interactions = \"spam ~ word_freq_free + word_freq_order + word_freq_receive + char_freq_hashtag * char_freq_exclamation + word_freq_telnet + word_freq_technology + word_freq_conference * word_freq_edu + word_freq_hp + word_freq_money + word_freq_credit + word_freq_000 + char_freq_dollar + word_freq_your * word_freq_email * word_freq_address + word_freq_people + word_freq_mail + word_freq_george + word_freq_our * word_freq_meeting\"\n",
    "\n",
    "myprobit = sm.Probit.from_formula(formula=formula_interactions, data=spam).fit()\n",
    "\n",
    "p = myprobit.pvalues\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2cf4f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_our:word_freq_meeting\n",
      "word_freq_our:word_freq_meeting\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.268018\n",
      "         Iterations 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2524.3003032611086\n",
      "word_freq_receive\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.268049\n",
      "         Iterations 13\n",
      "2522.5914836613906\n",
      "word_freq_conference:word_freq_edu\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.268100\n",
      "         Iterations 13\n",
      "2521.054837918212\n",
      "word_freq_telnet\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.268471\n",
      "         Iterations 13\n",
      "2522.4738070364815\n",
      "word_freq_people\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.268673\n",
      "         Iterations 13\n",
      "2522.325516194447\n",
      "word_freq_your:word_freq_email\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.269005\n",
      "         Iterations 13\n",
      "2523.385000656226\n",
      "char_freq_hashtag\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.269430\n",
      "         Iterations 13\n",
      "2525.290846978696\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while any(p > 0.05):\n",
    "    worstp = p.idxmax()\n",
    "    print(worstp)\n",
    "\n",
    "    if i == 1:\n",
    "        # Remove the outcome variable from the formula\n",
    "        formula_interactions = formula_interactions.replace(\"spam ~ \", \"\")\n",
    "\n",
    "        # Create the design matrix with interaction terms\n",
    "        Z = patsy.dmatrix(formula_interactions, data=spam)\n",
    "\n",
    "        # Convert the design matrix to a DataFrame\n",
    "        Z = pd.DataFrame(Z, columns=Z.design_info.column_names)\n",
    "        i=2\n",
    "    else:\n",
    "        Z = Z.drop(worstp, axis=1)\n",
    "        Z_names = ['Intercept'] + list(Z.columns)[1:]\n",
    "        Z.columns = Z_names\n",
    "\n",
    "        myprobit = sm.Probit(spam['spam'], Z).fit()\n",
    "\n",
    "        p = myprobit.pvalues\n",
    "        print(myprobit.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3f2b79fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Probit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                   spam   No. Observations:                 4601\n",
      "Model:                         Probit   Df Residuals:                     4578\n",
      "Method:                           MLE   Df Model:                           22\n",
      "Date:                Sun, 18 Jun 2023   Pseudo R-squ.:                  0.5982\n",
      "Time:                        22:38:30   Log-Likelihood:                -1239.6\n",
      "converged:                       True   LL-Null:                       -3085.1\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "====================================================================================================================\n",
      "                                                       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                           -0.7396      0.046    -16.142      0.000      -0.829      -0.650\n",
      "word_freq_free                                       0.6067      0.056     10.793      0.000       0.497       0.717\n",
      "word_freq_order                                      0.4560      0.110      4.156      0.000       0.241       0.671\n",
      "char_freq_exclamation                                0.1254      0.024      5.322      0.000       0.079       0.172\n",
      "char_freq_hashtag:char_freq_exclamation             16.1087      3.000      5.370      0.000      10.230      21.988\n",
      "word_freq_technology                                 0.4520      0.141      3.216      0.001       0.177       0.727\n",
      "word_freq_conference                                -1.7576      0.554     -3.172      0.002      -2.844      -0.671\n",
      "word_freq_edu                                       -0.8706      0.108     -8.055      0.000      -1.082      -0.659\n",
      "word_freq_hp                                        -1.0880      0.096    -11.377      0.000      -1.275      -0.901\n",
      "word_freq_money                                      0.3329      0.062      5.400      0.000       0.212       0.454\n",
      "word_freq_credit                                     0.8436      0.185      4.568      0.000       0.482       1.206\n",
      "word_freq_000                                        1.6985      0.203      8.374      0.000       1.301       2.096\n",
      "char_freq_dollar                                     2.9527      0.265     11.151      0.000       2.434       3.472\n",
      "word_freq_your                                       0.1546      0.023      6.856      0.000       0.110       0.199\n",
      "word_freq_email                                      0.1901      0.064      2.963      0.003       0.064       0.316\n",
      "word_freq_address                                   -0.1223      0.051     -2.414      0.016      -0.222      -0.023\n",
      "word_freq_your:word_freq_address                     0.1643      0.064      2.579      0.010       0.039       0.289\n",
      "word_freq_email:word_freq_address                    0.6467      0.196      3.293      0.001       0.262       1.032\n",
      "word_freq_your:word_freq_email:word_freq_address    -0.1923      0.044     -4.365      0.000      -0.279      -0.106\n",
      "word_freq_mail                                       0.0972      0.036      2.729      0.006       0.027       0.167\n",
      "word_freq_george                                    -3.1495      0.503     -6.261      0.000      -4.135      -2.164\n",
      "word_freq_our                                        0.3666      0.042      8.758      0.000       0.285       0.449\n",
      "word_freq_meeting                                   -1.4910      0.360     -4.137      0.000      -2.197      -0.785\n",
      "====================================================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.24 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "print(myprobit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0acdff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.670523\n",
      "         Iterations 4\n",
      "Probitlikelihood ratio test p-value: 0.0\n",
      "LinkTest result                  Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 4601\n",
      "Model:                            GLM   Df Residuals:                     4598\n",
      "Model Family:                Binomial   Df Model:                            2\n",
      "Link Function:                 probit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                    nan\n",
      "Date:                Sun, 18 Jun 2023   Deviance:                       57288.\n",
      "Time:                        22:38:30   Pearson chi2:                 2.80e+18\n",
      "No. Iterations:                    30   Pseudo R-squ. (CS):                nan\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       -2.89e+13   1.35e+05  -2.14e+08      0.000   -2.89e+13   -2.89e+13\n",
      "x1          4.013e+13   1.49e+04    2.7e+09      0.000    4.01e+13    4.01e+13\n",
      "x2          7.618e+11    642.395   1.19e+09      0.000    7.62e+11    7.62e+11\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justy\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1014: RuntimeWarning: divide by zero encountered in log\n",
      "  special.gammaln(n - y + 1) + y * np.log(mu / (1 - mu + 1e-20)) +\n",
      "c:\\Users\\justy\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1014: RuntimeWarning: invalid value encountered in multiply\n",
      "  special.gammaln(n - y + 1) + y * np.log(mu / (1 - mu + 1e-20)) +\n"
     ]
    }
   ],
   "source": [
    "null_probit = sm.Probit(spam[\"spam\"], sm.add_constant(pd.Series([1] * len(spam)))).fit()\n",
    "logit_lrtest = stats.chi2.sf(2 * (myprobit.llf - null_probit.llf), 1)\n",
    "print(\"Probitlikelihood ratio test p-value:\", probit_lrtest)\n",
    "\n",
    "linktest_result_probit = linktest_probit(myprobit)\n",
    "print(\"LinkTest result\", linktest_result_probit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d4f9ae4",
   "metadata": {},
   "source": [
    "Bot yhat and yhat squared are significant - the specification of the model is not correct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e005aadc",
   "metadata": {},
   "source": [
    "## As in previous study the Link Test and LR test draw attention to the logit model with interactions that was well specified and significant\n",
    "### Now let us perform other tests for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e5548503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodness-of-Fit Test:\n",
      "Deviance:  1827.0817098338612\n"
     ]
    }
   ],
   "source": [
    "gof_results = mylogit.deviance\n",
    "print(\"Goodness-of-Fit Test:\")\n",
    "print(\"Deviance: \", gof_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22edd22d",
   "metadata": {},
   "source": [
    "## Hosmer-Lemershow test - for specification\n",
    "## Source: https://stackoverflow.com/questions/40327399/hosmer-lemeshow-goodness-of-fit-test-in-python\n",
    "### Results are in line with our calculations performed originally in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d80f9b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HL test: 0.0\n"
     ]
    }
   ],
   "source": [
    "predictions = mylogit.predict()\n",
    "Y = mylogit.model.endog\n",
    "\n",
    "hl_df = pd.DataFrame({\n",
    "\n",
    "\"P_i\": predictions,\n",
    "\"Outcome\": Y\n",
    "\n",
    "})\n",
    "\n",
    "hl_df[\"decile\"] = pd.qcut(hl_df[\"P_i\"],10)\n",
    "obsevents_1 = hl_df[\"Outcome\"].groupby(hl_df.decile).sum()\n",
    "obsevents_0 = hl_df[\"Outcome\"].groupby(hl_df.decile).count() - obsevents_1\n",
    "expevents_1 = hl_df[\"P_i\"].groupby(hl_df.decile).sum()\n",
    "expevents_0 = hl_df[\"P_i\"].groupby(hl_df.decile).count() - expevents_1\n",
    "hl = (((obsevents_0 - expevents_0)**2)/(expevents_0)).sum() + (((obsevents_1 - expevents_1)**2)/(expevents_1)).sum()\n",
    "pvalue = 1 - chi2.cdf(hl , 10 - 2)\n",
    "\n",
    "print('HL test:', pvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3845d367",
   "metadata": {},
   "source": [
    "## Osius-Rojek Test - the only part of code that could not be reproduced as we were not able to find the function in Python nor write with help of the Internet\n",
    "\n",
    "### The code visible in the bottom were produced by chat gpt based on the description in R function:\n",
    "\n",
    "Osius and Rojek's tests\n",
    "These are based on a power-divergence statistic PD[l] (l=1 for Pearsons test) and the standard deviation (herein, of a binomial distribution) SD. The statistic is:\n",
    "\n",
    "Z[OR] = PD[l] - lbar / SD[l]\n",
    "\n",
    "\n",
    "For logistic regression, it is calculated as:\n",
    "\n",
    "Z[OR] = (chiSq - (n - p)) / (2 * n * SUM 1/n[i])^0.5\n",
    "\n",
    "where RSS is the residual sum-of-squares from a weighted linear regression:\n",
    "\n",
    "(1 - 2 * P[i]) / SD[i] ~ X, weights = SD[i]\n",
    "\n",
    "Here \\bold{X} is the matrix of model predictors.\n",
    "A two-tailed test against a standard normal distribution N ~ (0, 1) should not be significant.\n",
    "\n",
    "Source: Osius G & Rojek D, 1992. Normal goodness-of-fit tests for multinomial models with large degrees of freedom. Journal of the American Statistical Association. 87(420):1145-52. doi: 10.1080/01621459.1992.10476271. Also available at JSTOR at https://www.jstor.org/stable/2290653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6adf25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Statistic:  202.4603855218882\n",
      "P-value:  0.0\n"
     ]
    }
   ],
   "source": [
    "result = mylogit\n",
    "pred_probs = result.predict()\n",
    "\n",
    "epsilon = 1e-12\n",
    "pred_probs = np.clip(pred_probs, epsilon, 1-epsilon)\n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = (1 - 2 * pred_probs) / np.sqrt(pred_probs * (1 - pred_probs))\n",
    "\n",
    "# Calculate the standard deviation of a binomial distribution (SD)\n",
    "sd = np.sqrt(np.mean(pred_probs * (1 - pred_probs)))\n",
    "\n",
    "# Calculate the residual sum of squares (RSS)\n",
    "rss = np.sum(residuals**2)\n",
    "\n",
    "# Calculate the chi-square value\n",
    "chi_sq = result.llf * -2\n",
    "\n",
    "# Calculate the test statistic Z[OR]\n",
    "n = len(result.model.endog)\n",
    "p = result.df_model\n",
    "test_statistic = (chi_sq - p) / np.sqrt(2 * p)\n",
    "\n",
    "## Value obatined from R - good one\n",
    "#test_statistic = 0.1730284\n",
    "\n",
    "# Calculate the p-value based on a two-tailed test against a standard normal distribution\n",
    "p_value = 1 - stats.norm.cdf(test_statistic)\n",
    "\n",
    "print(\"Test Statistic: \", test_statistic)\n",
    "print(\"P-value: \", p_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f3025fa",
   "metadata": {},
   "source": [
    "## McFadden pseudo R-squared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f5ff822",
   "metadata": {},
   "source": [
    "McFadden's Pseudo-R-squared measures how well our model fits the data by calculating the ratio of the log likelihood for the model and an intercept-only model. The result is presented by subtracting the result from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9e9ee9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo R-squared:  0.7038838814093342\n"
     ]
    }
   ],
   "source": [
    "# Pseudo R-squared\n",
    "pseudo_r2 = 1 - (mylogit.llf / mylogit.llnull)\n",
    "print(\"Pseudo R-squared: \", pseudo_r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8028d887",
   "metadata": {},
   "source": [
    "We also calculated the count statistic, which was fully in line with the outcomes produced in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "493ef8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Pseudo R-squared:  0.9280591175831341\n"
     ]
    }
   ],
   "source": [
    "pred_probs = result.predict()\n",
    "y_observed = result.model.endog\n",
    "\n",
    "# Round predicted probabilities to 0 or 1 based on the cutoff of 0.50\n",
    "pred_classes = np.where(pred_probs > 0.50, 1, 0)\n",
    "\n",
    "# Calculate the number of correctly classified cases\n",
    "correctly_classified = np.sum(pred_classes == y_observed)\n",
    "\n",
    "# Calculate the total number of cases\n",
    "total_cases = len(y_observed)\n",
    "\n",
    "# Calculate the count pseudo R-squared\n",
    "count_r2 = correctly_classified / total_cases\n",
    "\n",
    "# Calculate the number of predictors in the model\n",
    "p = result.df_model\n",
    "\n",
    "# Print the pseudo R-squared statistics\n",
    "print(\"Count Pseudo R-squared: \", count_r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06e20c2b",
   "metadata": {},
   "source": [
    "## Marginal effects\n",
    "Source: https://gist.github.com/BioSciEconomist/e5e6cd377ee8ccd565db967e76a58088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "783fadb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0008757521280584903"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# home grown marginal effects function for logit model with 2 variables\n",
    "\n",
    "def mfx(result,mu1,mu2,par):\n",
    "    \"\"\"\n",
    "    result: model object from stats models logistic regression\n",
    "    ex: y ~ b0 + b1*x1 + b2*x2\n",
    "    mu1: mean value for first variable in model\n",
    "    mu2: mean value for 2nd variable in model\n",
    "    par: indicates index from 0 for model parameter you want to convert to a \n",
    "         marginal effect\n",
    "    note: this easily extends to more variables but does not handle predictors\n",
    "          with multiple categories (unless they are dummy coded)\n",
    "    \"\"\"\n",
    "    b0 =  result.params[0]  \n",
    "    b1 =  result.params[1] \n",
    "    b2 =  result.params[2] \n",
    "    XB = mu1*b1 + mu2*b2 + b0 \n",
    "    return (np.exp(XB)/((1+np.exp(XB))**2))*result.params[par]\n",
    "\n",
    "\n",
    "mfx(result,.5,30,1) # home grown function gives almost\n",
    "                    # exact same result as get_margeff() above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7b501022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfx(result, *means, par):\n",
    "    \"\"\"\n",
    "    result: model object from statsmodels logistic regression\n",
    "    means: mean values for each variable in the model (in the same order as the coefficients)\n",
    "    par: index of the model parameter you want to convert to a marginal effect\n",
    "    \"\"\"\n",
    "    b0 = result.params[0]\n",
    "    params = result.params[1:]  # Exclude the intercept\n",
    "    XB = b0 + np.dot(params, means)\n",
    "    exp_XB = np.exp(XB)\n",
    "    marginal_effect = (exp_XB / ((1 + exp_XB) ** 2)) * result.params[par]\n",
    "    return marginal_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0bd3da4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.210805\n",
      "         Iterations 14\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                   spam   No. Observations:                 4601\n",
      "Model:                          Logit   Df Residuals:                     4563\n",
      "Method:                           MLE   Df Model:                           37\n",
      "Date:                Sun, 18 Jun 2023   Pseudo R-squ.:                  0.6856\n",
      "Time:                        22:39:13   Log-Likelihood:                -969.92\n",
      "converged:                       True   LL-Null:                       -3085.1\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=============================================================================================================\n",
      "                                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                    -1.2451      0.107    -11.688      0.000      -1.454      -1.036\n",
      "word_freq_make                               -0.4832      0.207     -2.332      0.020      -0.889      -0.077\n",
      "word_freq_address                            -0.1490      0.064     -2.328      0.020      -0.275      -0.024\n",
      "word_freq_our                                 0.5541      0.106      5.217      0.000       0.346       0.762\n",
      "word_freq_address_word_freq_our               0.9072      0.355      2.554      0.011       0.211       1.603\n",
      "word_freq_over                                0.7683      0.246      3.119      0.002       0.285       1.251\n",
      "word_freq_remove                              2.5287      0.347      7.294      0.000       1.849       3.208\n",
      "word_freq_internet                            0.5184      0.152      3.410      0.001       0.220       0.816\n",
      "word_freq_order                               1.6759      0.333      5.038      0.000       1.024       2.328\n",
      "word_freq_free                                1.1187      0.152      7.347      0.000       0.820       1.417\n",
      "word_freq_order_word_freq_free               -1.4575      0.807     -1.807      0.071      -3.039       0.124\n",
      "word_freq_business                            1.2109      0.236      5.120      0.000       0.747       1.674\n",
      "word_freq_order_word_freq_business           -0.9479      0.161     -5.886      0.000      -1.264      -0.632\n",
      "word_freq_you                                 0.0545      0.033      1.628      0.103      -0.011       0.120\n",
      "word_freq_credit                              2.6087      0.633      4.121      0.000       1.368       3.849\n",
      "word_freq_your                                0.2238      0.048      4.699      0.000       0.130       0.317\n",
      "word_freq_credit_word_freq_your              -0.5850      0.249     -2.345      0.019      -1.074      -0.096\n",
      "word_freq_000                                 2.4533      0.481      5.095      0.000       1.510       3.397\n",
      "word_freq_money                               0.6090      0.207      2.945      0.003       0.204       1.014\n",
      "word_freq_hp                                 -2.2200      0.279     -7.958      0.000      -2.767      -1.673\n",
      "word_freq_hpl                                -1.3290      0.462     -2.873      0.004      -2.235      -0.423\n",
      "word_freq_george                             -9.3934      2.108     -4.455      0.000     -13.526      -5.261\n",
      "word_freq_650                                 0.5403      0.246      2.197      0.028       0.058       1.022\n",
      "word_freq_data                               -0.5942      0.258     -2.304      0.021      -1.100      -0.089\n",
      "word_freq_85                                 -1.8168      0.952     -1.908      0.056      -3.683       0.049\n",
      "word_freq_technology                          0.9090      0.319      2.852      0.004       0.284       1.534\n",
      "word_freq_pm                                 -1.3025      0.393     -3.318      0.001      -2.072      -0.533\n",
      "word_freq_meeting                            -2.7385      0.788     -3.474      0.001      -4.283      -1.194\n",
      "word_freq_project                            -2.4918      0.579     -4.300      0.000      -3.628      -1.356\n",
      "word_freq_re                                 -0.8653      0.152     -5.711      0.000      -1.162      -0.568\n",
      "word_freq_edu                                -1.6046      0.271     -5.927      0.000      -2.135      -1.074\n",
      "word_freq_conference                         -2.1460      0.973     -2.206      0.027      -4.053      -0.239\n",
      "char_freq_exclamation                         0.2892      0.072      4.008      0.000       0.148       0.431\n",
      "char_freq_semicolon_char_freq_exclamation     3.7338      1.211      3.082      0.002       1.360       6.108\n",
      "char_freq_dollar                              3.4195      0.712      4.801      0.000       2.024       4.815\n",
      "char_freq_exclamation_char_freq_dollar       16.7776      4.071      4.122      0.000       8.799      24.756\n",
      "char_freq_hashtag                             3.7910      0.534      7.103      0.000       2.745       4.837\n",
      "char_freq_semicolon_char_freq_hashtag        -4.3781      1.981     -2.210      0.027      -8.261      -0.495\n",
      "=============================================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.25 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n",
      "        Logit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:                   spam\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "=============================================================================================================\n",
      "                                               dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "word_freq_make                               -0.0321      0.014     -2.342      0.019      -0.059      -0.005\n",
      "word_freq_address                            -0.0099      0.004     -2.330      0.020      -0.018      -0.002\n",
      "word_freq_our                                 0.0368      0.007      5.339      0.000       0.023       0.050\n",
      "word_freq_address_word_freq_our               0.0603      0.024      2.559      0.010       0.014       0.106\n",
      "word_freq_over                                0.0510      0.016      3.139      0.002       0.019       0.083\n",
      "word_freq_remove                              0.1680      0.022      7.557      0.000       0.124       0.212\n",
      "word_freq_internet                            0.0344      0.010      3.443      0.001       0.015       0.054\n",
      "word_freq_order                               0.1113      0.022      5.133      0.000       0.069       0.154\n",
      "word_freq_free                                0.0743      0.010      7.642      0.000       0.055       0.093\n",
      "word_freq_order_word_freq_free               -0.0968      0.053     -1.811      0.070      -0.202       0.008\n",
      "word_freq_business                            0.0804      0.015      5.214      0.000       0.050       0.111\n",
      "word_freq_order_word_freq_business           -0.0630      0.010     -6.033      0.000      -0.083      -0.043\n",
      "word_freq_you                                 0.0036      0.002      1.632      0.103      -0.001       0.008\n",
      "word_freq_credit                              0.1733      0.042      4.162      0.000       0.092       0.255\n",
      "word_freq_your                                0.0149      0.003      4.795      0.000       0.009       0.021\n",
      "word_freq_credit_word_freq_your              -0.0389      0.017     -2.353      0.019      -0.071      -0.006\n",
      "word_freq_000                                 0.1630      0.031      5.181      0.000       0.101       0.225\n",
      "word_freq_money                               0.0405      0.014      2.965      0.003       0.014       0.067\n",
      "word_freq_hp                                 -0.1475      0.018     -8.238      0.000      -0.183      -0.112\n",
      "word_freq_hpl                                -0.0883      0.031     -2.884      0.004      -0.148      -0.028\n",
      "word_freq_george                             -0.6240      0.139     -4.501      0.000      -0.896      -0.352\n",
      "word_freq_650                                 0.0359      0.016      2.203      0.028       0.004       0.068\n",
      "word_freq_data                               -0.0395      0.017     -2.308      0.021      -0.073      -0.006\n",
      "word_freq_85                                 -0.1207      0.063     -1.911      0.056      -0.244       0.003\n",
      "word_freq_technology                          0.0604      0.021      2.871      0.004       0.019       0.102\n",
      "word_freq_pm                                 -0.0865      0.026     -3.340      0.001      -0.137      -0.036\n",
      "word_freq_meeting                            -0.1819      0.052     -3.491      0.000      -0.284      -0.080\n",
      "word_freq_project                            -0.1655      0.038     -4.341      0.000      -0.240      -0.091\n",
      "word_freq_re                                 -0.0575      0.010     -5.821      0.000      -0.077      -0.038\n",
      "word_freq_edu                                -0.1066      0.018     -6.038      0.000      -0.141      -0.072\n",
      "word_freq_conference                         -0.1426      0.064     -2.211      0.027      -0.269      -0.016\n",
      "char_freq_exclamation                         0.0192      0.005      4.062      0.000       0.010       0.028\n",
      "char_freq_semicolon_char_freq_exclamation     0.2481      0.080      3.103      0.002       0.091       0.405\n",
      "char_freq_dollar                              0.2272      0.047      4.885      0.000       0.136       0.318\n",
      "char_freq_exclamation_char_freq_dollar        1.1146      0.268      4.154      0.000       0.589       1.640\n",
      "char_freq_hashtag                             0.2519      0.034      7.311      0.000       0.184       0.319\n",
      "char_freq_semicolon_char_freq_hashtag        -0.2909      0.131     -2.215      0.027      -0.548      -0.033\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "X['spam'] = spam['spam']\n",
    "last_column = X.columns[-1]  # Get the name of the last column\n",
    "columns = [last_column] + list(X.columns[:-1])  # Create a new list of column names with the last column at the front\n",
    "X = X[columns]  # Reorder the columns in the DataFrame\n",
    "\n",
    "# Replace the colon with a multiplication sign in the column name\n",
    "X.rename(columns={'char_freq_semicolon:char_freq_exclamation' : 'char_freq_semicolon_char_freq_exclamation'}, inplace=True)\n",
    "X.rename(columns={'char_freq_semicolon:char_freq_hashtag' : 'char_freq_semicolon_char_freq_hashtag'}, inplace=True)\n",
    "X.rename(columns={'word_freq_credit:word_freq_your' : 'word_freq_credit_word_freq_your'}, inplace=True)\n",
    "X.rename(columns={'word_freq_address:word_freq_our' : 'word_freq_address_word_freq_our'}, inplace=True)\n",
    "X.rename(columns={'word_freq_order:word_freq_free' : 'word_freq_order_word_freq_free'}, inplace=True)\n",
    "X.rename(columns={'word_freq_order:word_freq_business' : 'word_freq_order_word_freq_business'}, inplace=True)\n",
    "X.rename(columns={'char_freq_exclamation:char_freq_dollar' : 'char_freq_exclamation_char_freq_dollar'}, inplace=True)\n",
    "\n",
    "epsilon = 1e-8\n",
    "\n",
    "X = X.replace(0, epsilon)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = smf.logit(formula='spam ~ word_freq_make+word_freq_address+word_freq_our+word_freq_address_word_freq_our+word_freq_over+word_freq_remove+word_freq_internet+word_freq_order+word_freq_free+word_freq_order_word_freq_free+word_freq_business+word_freq_order_word_freq_business+word_freq_you+word_freq_credit+word_freq_your+word_freq_credit_word_freq_your+word_freq_000+word_freq_money+word_freq_hp+word_freq_hpl+word_freq_george+word_freq_650+word_freq_data+word_freq_85+word_freq_technology+word_freq_pm+word_freq_meeting+word_freq_project+word_freq_re+word_freq_edu+word_freq_conference+char_freq_exclamation+char_freq_semicolon_char_freq_exclamation+char_freq_dollar+char_freq_exclamation_char_freq_dollar+char_freq_hashtag+char_freq_semicolon_char_freq_hashtag', data=X).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# Get marginal effects\n",
    "margeff_overall = model.get_margeff(at='overall')\n",
    "print(margeff_overall.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dfafa57",
   "metadata": {},
   "source": [
    "The marginal effects were a little different due to the fact that we have to exclude 2 variables that could not be used in Python function. However, the results were relatively simillar, with the same sing of coefficients and signifficance of p-values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "301b1335",
   "metadata": {},
   "source": [
    "# Spam Detection Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18f984ae",
   "metadata": {},
   "source": [
    "* Data exploration - let us check how the columns look like and delete unnecessary ones and then move to preprocessing\n",
    "* We dont need the first column 'Unnamed: 0' as it is just an ID\n",
    "* The columns 'label' and 'label_num' depict the same thing, let us only leave 'label_num' that represent spam (1) and non-spam (0) label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0299e62d",
   "metadata": {},
   "source": [
    "Import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a486ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\serei\\Downloads\\spam.csv\", encoding = \"latin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68214b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"v1\", \"v2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5562a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57893f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename({\"v1\": \"spam\", \"v2\":\"text\"}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e78effa0",
   "metadata": {},
   "source": [
    "Data preprocessing - we only need to take care of the text column\n",
    "Convert text to lowercase - the iteration is required to go through each mail (it seems that the text was already in lower case but just to make sure let us do it again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'][0]\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    data['text'][i] = data['text'][i].lower()\n",
    "\n",
    "## Check if it worked\n",
    "\n",
    "data['text'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "196f89e2",
   "metadata": {},
   "source": [
    "## Word Frequency\n",
    "\n",
    "The previous analysis was based on word frequencies appearing in the text. In order to obtain the right word frequencies, we have to remove the stopwords as well as conduct lemmantization to obtain the root of the word and reduce the number of unique words. \n",
    "Next, we count the occurence of the top 50 most frequent words, special characters and numbers in each email.\n",
    "\n",
    "We generate the functions with the help of ChatGPT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f7e8d85",
   "metadata": {},
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into individual words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize each word and remove stop words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the lemmatized words back into a single string\n",
    "    processed_text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['processed_text'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94ce9ab9",
   "metadata": {},
   "source": [
    "The words are tokenized excluding the numbers (as random selection of integers were appearing in the top 50). We obtain the 50 most frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into words while excluding numbers\n",
    "all_text = data['processed_text'].str.cat(sep=' ')\n",
    "words = nltk.word_tokenize(all_text)\n",
    "words = [word for word in words if word.isalpha()]\n",
    "\n",
    "# Create a frequency distribution of the words\n",
    "freq_dist = FreqDist(words)\n",
    "\n",
    "# Retrieve the most common words\n",
    "num_most_common = 50\n",
    "most_common_words = freq_dist.most_common(num_most_common)\n",
    "\n",
    "# Print the most common words and their frequencies\n",
    "for word, frequency in most_common_words:\n",
    "    print(word, frequency)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef073b64",
   "metadata": {},
   "source": [
    "A list of only the needed words is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb9df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [word for word, count in most_common_words]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d76d8f9",
   "metadata": {},
   "source": [
    "We generate a word frequency matrix for the most common words, as well as adding additional columns for special characters and integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=word_list)\n",
    "X = vectorizer.fit_transform(data[\"processed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfdd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "data = pd.concat([data[[\"processed_text\", \"spam\"]], matrix], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in [\"$\", \"€\", \"!\", \"@\", \"?\"]:\n",
    "    pattern = re.escape(char)  # Escape special characters in the regex pattern\n",
    "    data[char] = data[\"processed_text\"].str.count(pattern)\n",
    "\n",
    "data[\"digit_count\"] = data[\"processed_text\"].str.count(r\"\\d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da60a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = data.columns[2:57]\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n",
    "\n",
    "# Display the scaled DataFrame\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"C:\\Users\\serei\\Desktop\\Untitled Folder\\new_data_new_words.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "599256e8",
   "metadata": {},
   "source": [
    "# Word frequencies from previous analysis\n",
    "\n",
    "We also want to create a word frequency matrix with the words used in the previous analysis. We calculate the frequencies as well as dividing them by the total number of words in the text to match the previous dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_old = pd.DataFrame(data[[\"processed_text\", \"spam\"]])\n",
    "\n",
    "words_old = ['make', 'address', 'all', '3d', 'our', 'over', 'remove', 'internet','order',\n",
    "             'mail', 'receive', 'will', 'people', 'report', 'addresses','free', 'business',\n",
    "             'email', 'you', 'credit', 'your', 'font', '000', 'money', 'hp','hpl','george',\n",
    "             '650', 'lab', 'labs', 'telnet', '857', 'data', '415', '85', 'technology', '1999', \n",
    "             'parts', 'pm', 'direct', 'cs', 'meeting', 'original', 'project', 're', 'edu', 'table',\n",
    "             'conference', ';', '(', '[', '!', '$', '#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381dcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in words_old:\n",
    "    pattern = re.escape(char)  # Escape special characters in the regex pattern\n",
    "    data_old[char] = data[\"processed_text\"].str.count(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_old[\"word_count\"] = data_old[\"processed_text\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_old[words_old] = data_old[words_old].div(data_old[\"word_count\"], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3888848",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_old.to_csv(r\"C:\\Users\\serei\\Desktop\\Untitled Folder\\new_data_old_words.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e861fe72",
   "metadata": {},
   "source": [
    "# Spam Detection Project Reproduce new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e549fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import patsy\n",
    "from statsmodels.discrete.discrete_model import ProbitResults, LogitResults\n",
    "os.chdir(r\"C:\\Users\\serei\\Desktop\\Untitled Folder\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23adaba6",
   "metadata": {},
   "source": [
    "The analysis will be conducted using word frequencies from the old research as well as a new word list from the new datsaset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc02d7cd",
   "metadata": {},
   "source": [
    "## New Data New Words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f69a7c26",
   "metadata": {},
   "source": [
    "Import and prepare the dataset for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e374216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv(\"new_data_new_words.csv\")\n",
    "spam.dropna(inplace = True)\n",
    "\n",
    "#Renaming character columns to a less error-prone form\n",
    "spam.rename(columns = {'$':'dollar',\n",
    "                       '!': 'exclamation',\n",
    "                      \"#\": \"hashtag\",\n",
    "                       \"(\":\"parenthesis\",\n",
    "                       \"[\": \"brackets\",\n",
    "                       \";\": \"semicolon\",\n",
    "                       \"€\": \"euro\",\n",
    "                       \"@\": \"at\", \n",
    "                       \"?\": \"question\"\n",
    "                      }, inplace = True)\n",
    "\n",
    "#Drop columns not neccessary to the analysis\n",
    "spam.drop(columns=['Unnamed: 0', 'processed_text', 'word_count'], inplace = True)\n",
    "\n",
    "#Rename columns to include word_freq\n",
    "column_names = spam.columns.tolist()\n",
    "new_column_names = ['spam'] + ['word_freq_' + column if column != 'spam' else column for column in column_names[1:]]\n",
    "\n",
    "spam.rename(columns=dict(zip(column_names, new_column_names)), inplace=True)\n",
    "\n",
    "#Convert the dependent variable to a numeric one\n",
    "spam['spam'] = spam['spam'].replace({\n",
    "    'spam': 1,\n",
    "    'ham': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.describe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf354f49",
   "metadata": {},
   "source": [
    "Very low variances in independent variables cause errors in the models. We set a treshold for the variances of the columns and keep only the columns that can be included in the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87311ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = spam.var()\n",
    "\n",
    "# Set the threshold value\n",
    "threshold = 0.003\n",
    "\n",
    "# Filter columns based on variance threshold\n",
    "filtered_columns = variance[variance >= threshold].index\n",
    "\n",
    "# Create a new DataFrame with selected columns\n",
    "spam = spam[filtered_columns]\n",
    "\n",
    "spam.var()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbc2b2c5",
   "metadata": {},
   "source": [
    "Let us check the distribution of spam and non-spam mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_count = spam[\"spam\"].value_counts()\n",
    "perc_yes = round(spam_count[1] / len(spam) * 100, 2)\n",
    "perc_no = round(spam_count[0] / len(spam) * 100, 2)\n",
    "\n",
    "plt.bar([\"Non-spam\", \"Spam\"], spam_count, color=[\"#31d64f\", \"#ed3b3b\"])\n",
    "plt.title(\"Number of spam/non-spam mails\")\n",
    "plt.xlabel(\"Type of mail\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend([\"Non-spam\", \"Spam\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ca044cc",
   "metadata": {},
   "source": [
    "## Start from the most general model that contains all explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"spam ~ \" + \" + \".join(spam.columns[1:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f30798a6",
   "metadata": {},
   "source": [
    "### Probit model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "myprobit = sm.Probit.from_formula(formula, data=spam).fit()\n",
    "print(myprobit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b061674",
   "metadata": {},
   "source": [
    "### Logit model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylogit = sm.Logit.from_formula(formula, data=spam).fit()\n",
    "print(mylogit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d3d86db",
   "metadata": {},
   "source": [
    "# Significance test of models\n",
    "\n",
    "Both models p-values are 0, so null hypothesis can be rejected. It means that the model`s coefficients are jointly significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_probit = sm.Probit(spam[\"spam\"], sm.add_constant(pd.Series([1] * len(spam)))).fit()\n",
    "probit_lrtest = stats.chi2.sf(2 * (myprobit.llf - null_probit.llf), 1)\n",
    "print(\"Probit likelihood ratio test p-value:\", probit_lrtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe87a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_logit = sm.Logit(spam[\"spam\"], sm.add_constant(pd.Series([1] * len(spam)))).fit()\n",
    "logit_lrtest = stats.chi2.sf(2 * (mylogit.llf - null_logit.llf), 1)\n",
    "print(\"Logit likelihood ratio test p-value:\", logit_lrtest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39dd4f5a",
   "metadata": {},
   "source": [
    "## Stepwise regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11906d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_probit = myprobit.pvalues\n",
    "spam_temp_probit = spam.copy()\n",
    "\n",
    "while any(p_probit > 0.05):\n",
    "    worstp = p_probit.idxmax()\n",
    "    \n",
    "    print(worstp)\n",
    "    spam_temp_probit.drop(columns=worstp, inplace=True)\n",
    "    \n",
    "    formula = \"spam ~\"\n",
    "    \n",
    "    for column in spam_temp_probit.columns[1:]:\n",
    "        formula += f\" + {column}\"\n",
    "    \n",
    "    myprobit = sm.Probit.from_formula(formula, data=spam_temp_probit).fit()\n",
    "    p_probit = myprobit.pvalues\n",
    "    print(myprobit.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513453b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_logit = mylogit.pvalues\n",
    "spam_temp_logit = spam.copy()\n",
    "\n",
    "while any(p_logit > 0.05):\n",
    "    worstp = p_logit.idxmax()\n",
    "    \n",
    "    print(worstp)\n",
    "    spam_temp_logit.drop(columns=worstp, inplace=True)\n",
    "    \n",
    "    formula = \"spam ~\"\n",
    "    \n",
    "    for column in spam_temp_logit.columns[1:]:\n",
    "        formula += f\" + {column}\"\n",
    "    \n",
    "    mylogit = sm.Logit.from_formula(formula, data=spam_temp_logit).fit()\n",
    "    p_logit = mylogit.pvalues\n",
    "    print(mylogit.aic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa614327",
   "metadata": {},
   "source": [
    "## Link test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1719c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linktest_probit(model):\n",
    "    \"\"\"\n",
    "    Function to perform linktest on a logistic regression model.\n",
    "    \n",
    "    Args:\n",
    "    - model: logistic regression model object\n",
    "    \n",
    "    Returns:\n",
    "    - aux_reg: auxiliary regression model object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the data\n",
    "    y = model.model.endog\n",
    "    pred = model.predict()\n",
    "    pred = np.clip(pred, 1e-12, 1 - 1e-12)\n",
    "    yhat = np.log(pred/(1-pred))\n",
    "    yhat2 = yhat**2\n",
    "\n",
    "    # Add constant column to predictor variables\n",
    "    X = np.column_stack((np.ones_like(y), yhat, yhat2))\n",
    "\n",
    "    # Remove rows with missing or infinite values\n",
    "    valid_idx = np.isfinite(X).all(axis=1)\n",
    "    X = X[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "\n",
    "    # Fit the binomial regression model\n",
    "    model = sm.GLM(y, X, family=sm.families.Binomial(link=sm.genmod.families.links.probit()))\n",
    "    result = model.fit()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def linktest_logit(model):\n",
    "    \"\"\"\n",
    "    Function to perform linktest on a logistic regression model.\n",
    "    \n",
    "    Args:\n",
    "    - model: logistic regression model object\n",
    "    \n",
    "    Returns:\n",
    "    - aux_reg: auxiliary regression model object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the data\n",
    "    y = model.model.endog\n",
    "    pred = model.predict()\n",
    "    pred = np.clip(pred, 1e-12, 1 - 1e-12)\n",
    "    yhat = np.log(pred/(1-pred))\n",
    "    yhat2 = yhat**2\n",
    "\n",
    "    # Add constant column to predictor variables\n",
    "    X = np.column_stack((np.ones_like(y), yhat, yhat2))\n",
    "\n",
    "    # Remove rows with missing or infinite values\n",
    "    valid_idx = np.isfinite(X).all(axis=1)\n",
    "    X = X[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "\n",
    "    # Fit the binomial regression model\n",
    "    model = sm.GLM(y, X, family=sm.families.Binomial(link=sm.genmod.families.links.logit()))\n",
    "    result = model.fit()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linktest for probit model - after stepwise regression\n",
    "linktest_result_probit = linktest_probit(myprobit)\n",
    "print(linktest_result_probit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linktest for logit model - after stepwise regression\n",
    "linktest_result_logit = linktest_logit(mylogit)\n",
    "print(linktest_result_logit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e119155",
   "metadata": {},
   "source": [
    "### Interaction terms\n",
    "Adding interaction terms and deleting insignificant ones for probit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_interactions = \"spam ~  word_freq_free * word_freq_exclamation  + word_freq_send \"\n",
    "myprobit = sm.Probit.from_formula(formula_interactions, data=spam).fit()\n",
    "p_probit = myprobit.pvalues\n",
    "spam_temp_probit = spam.copy()\n",
    "\n",
    "while any(p_probit > 0.05):\n",
    "    worstp = p_probit.idxmax()\n",
    "    \n",
    "    print(worstp)\n",
    "    spam_temp_probit.drop(columns=worstp, inplace=True)\n",
    "    \n",
    "    formula = formula_interactions\n",
    "    \n",
    "    for column in spam_temp_probit.columns[1:]:\n",
    "        formula += f\" + {column}\"\n",
    "    \n",
    "    myprobit = sm.Probit.from_formula(formula, data=spam_temp_probit).fit()\n",
    "    p_probit = myprobit.pvalues\n",
    "    print(myprobit.aic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab63f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylogit = sm.Logit.from_formula(formula_interactions, data=spam).fit()\n",
    "p_logit = mylogit.pvalues\n",
    "spam_temp_probit = spam.copy()\n",
    "\n",
    "while any(p_logit > 0.05):\n",
    "    worstp = p_logit.idxmax()\n",
    "    \n",
    "    print(worstp)\n",
    "    spam_temp_logit.drop(columns=worstp, inplace=True)\n",
    "    \n",
    "    formula = formula_interactions\n",
    "    \n",
    "    for column in spam_temp_logit.columns[1:]:\n",
    "        formula += f\" + {column}\"\n",
    "    \n",
    "    mylogit = sm.Logit.from_formula(formula, data=spam_temp_logit).fit()\n",
    "    p_logit = mylogit.pvalues\n",
    "    print(mylogit.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2312e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "probit_lrtest = stats.chi2.sf(2 * (myprobit.llf - null_probit.llf), 1)\n",
    "print(\"Probit likelihood ratio test p-value:\", probit_lrtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lrtest = stats.chi2.sf(2 * (mylogit.llf - null_logit.llf), 1)\n",
    "print(\"Logit likelihood ratio test p-value:\", logit_lrtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b80a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1841771c",
   "metadata": {},
   "source": [
    "## Recration from line 267 from R code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aba1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_interactions = \"spam ~  word_freq_free * word_freq_exclamation  + word_freq_send + word_freq_get*word_freq_ur + word_freq_still + word_freq_sorry\"\n",
    "\n",
    "mylogit = sm.formula.glm(formula_interactions, data=spam, family=sm.families.Binomial(sm.genmod.families.links.logit())).fit()\n",
    "\n",
    "p = mylogit.pvalues\n",
    "print(p)\n",
    "\n",
    "mylogit = sm.formula.glm(formula_interactions, data=spam, family=sm.families.Binomial(sm.genmod.families.links.probit())).fit()\n",
    "\n",
    "p = myprobit.pvalues\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ccf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "while any(p > 0.05):\n",
    "    worstp = p.idxmax()\n",
    "    print(worstp)\n",
    "\n",
    "    if i == 1:\n",
    "        # Remove the outcome variable from the formula\n",
    "        formula_interactions = formula_interactions.replace(\"spam ~ \", \"\")\n",
    "\n",
    "        # Create the design matrix with interaction terms\n",
    "        X = patsy.dmatrix(formula_interactions, data=spam)\n",
    "\n",
    "        # Convert the design matrix to a DataFrame\n",
    "        X = pd.DataFrame(X, columns=X.design_info.column_names)\n",
    "        i=2\n",
    "    else:\n",
    "        X = X.drop(worstp, axis=1)\n",
    "        X_names = ['Intercept'] + list(X.columns)[1:]\n",
    "        X.columns = X_names\n",
    "\n",
    "        myprobit = sm.Probit(spam['spam'], X).fit()\n",
    "\n",
    "        print(myprobit.summary())\n",
    "        p = myprobit.pvalues\n",
    "        print(myprobit.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "while any(p > 0.05):\n",
    "    worstp = p.idxmax()\n",
    "    print(worstp)\n",
    "\n",
    "    if i == 1:\n",
    "        # Remove the outcome variable from the formula\n",
    "        formula_interactions = formula_interactions.replace(\"spam ~ \", \"\")\n",
    "\n",
    "        # Create the design matrix with interaction terms\n",
    "        X = patsy.dmatrix(formula_interactions, data=spam)\n",
    "\n",
    "        # Convert the design matrix to a DataFrame\n",
    "        X = pd.DataFrame(X, columns=X.design_info.column_names)\n",
    "        i=2\n",
    "    else:\n",
    "        X = X.drop(worstp, axis=1)\n",
    "        X_names = ['Intercept'] + list(X.columns)[1:]\n",
    "        X.columns = X_names\n",
    "\n",
    "        mylogit = sm.GLM(spam['spam'], X, family=sm.families.Binomial(sm.families.links.logit())).fit()\n",
    "\n",
    "        print(mylogit.summary())\n",
    "        p = mylogit.pvalues\n",
    "        print(mylogit.aic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f35c2695",
   "metadata": {},
   "source": [
    "### Goodnes of fit tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "694ae980",
   "metadata": {},
   "source": [
    "gof_results = mylogit.deviance\n",
    "print(\"Logit | Goodness-of-Fit Test:\")\n",
    "print(\"Logit | Deviance: \", gof_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4194f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_r2 = 1 - (mylogit.llf / mylogit.llnull)\n",
    "print(\"Logit | Pseudo R-squared: \", pseudo_r2)\n",
    "\n",
    "pseudo_r2 = 1 - (myprobit.llf / myprobit.llnull)\n",
    "print(\"Probit | Pseudo R-squared: \", pseudo_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbe288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating marginal effects\n",
    "meff = smf.probit(formula_interactions, data=spam_previous_words).fit()\n",
    "print(\"Probit | Marginal Effects:\")\n",
    "print(meff.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8003849",
   "metadata": {},
   "source": [
    "# New Data Old Words\n",
    "\n",
    "In this part of the analysis we will try to recreate the previous research on the new data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_previous_words = pd.read_csv(\"new_data_old_words.csv\")\n",
    "spam_previous_words.dropna(inplace = True)\n",
    "\n",
    "#Renaming character columns to a less error-prone form\n",
    "spam_previous_words.rename(columns = {'$':'dollar',\n",
    "                       '!': 'exclamation',\n",
    "                      \"#\": \"hashtag\",\n",
    "                       \"(\":\"parenthesis\",\n",
    "                       \"[\": \"brackets\",\n",
    "                       \";\": \"semicolon\",\n",
    "                       \"€\": \"euro\",\n",
    "                       \"@\": \"at\", \n",
    "                       \"?\": \"question\"\n",
    "                      }, inplace = True)\n",
    "\n",
    "#Drop columns not neccessary to the analysis\n",
    "spam_previous_words.drop(columns=['Unnamed: 0', 'processed_text', 'word_count'], inplace = True)\n",
    "\n",
    "#Rename columns to include word_freq\n",
    "column_names = spam_previous_words.columns.tolist()\n",
    "new_column_names = ['spam'] + ['word_freq_' + column if column != 'spam' else column for column in column_names[1:]]\n",
    "\n",
    "spam_previous_words.rename(columns=dict(zip(column_names, new_column_names)), inplace=True)\n",
    "\n",
    "#Convert the dependent variable to a numeric one\n",
    "spam_previous_words['spam'] = spam_previous_words['spam'].replace({\n",
    "    'spam': 1,\n",
    "    'ham': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c13e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_previous_words.describe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72335d16",
   "metadata": {},
   "source": [
    "We once again remove the columns with very low variances to ensure that the selected models run smoothly. We decrease the treshold as the previously used words are less frequently found in the new dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = spam_previous_words.var()\n",
    "\n",
    "# Set the threshold value\n",
    "threshold = 0.00017\n",
    "\n",
    "# Filter columns based on variance threshold\n",
    "filtered_columns = variance[variance >= threshold].index\n",
    "\n",
    "# Create a new DataFrame with selected columns\n",
    "spam_previous_words = spam_previous_words[filtered_columns]\n",
    "\n",
    "spam_previous_words.var()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d970976d",
   "metadata": {},
   "source": [
    "## Start from the most general model that contains all explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e3578",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"spam ~ \" + \" + \".join(spam_previous_words.columns[1:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1ab2444",
   "metadata": {},
   "source": [
    "### Probit model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myprobit = sm.Probit.from_formula(formula, data=spam_previous_words).fit()\n",
    "print(myprobit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5904585",
   "metadata": {},
   "source": [
    "### Logit model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace24a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylogit = sm.Logit.from_formula(formula, data=spam_previous_words).fit()\n",
    "print(mylogit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8616469",
   "metadata": {},
   "source": [
    "# Significance test of models\n",
    "\n",
    "Both models p-values are below the 0.05 treshold, so the null hypothesis can be rejected. It means that the model`s coefficients are jointly significant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b289a9b0",
   "metadata": {},
   "source": [
    "### Stepwise regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_probit = sm.Probit(spam_previous_words[\"spam\"], sm.add_constant(pd.Series([1] * len(spam_previous_words)))).fit()\n",
    "probit_lrtest = stats.chi2.sf(2 * (myprobit.llf - null_probit.llf), 1)\n",
    "print(\"Probit likelihood ratio test p-value:\", probit_lrtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80487f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_logit = sm.Probit(spam_previous_words[\"spam\"], sm.add_constant(pd.Series([1] * len(spam_previous_words)))).fit()\n",
    "logit_lrtest = stats.chi2.sf(2 * (mylogit.llf - null_logit.llf), 1)\n",
    "print(\"Logit likelihood ratio test p-value:\", logit_lrtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_probit = myprobit.pvalues\n",
    "spam_temp_probit = spam_previous_words.copy()\n",
    "\n",
    "while any(p_probit > 0.05):\n",
    "    worstp = p_probit.idxmax()\n",
    "    \n",
    "    print(worstp)\n",
    "    spam_temp_probit.drop(columns=worstp, inplace=True)\n",
    "    \n",
    "    formula = \"spam ~\"\n",
    "    \n",
    "    for column in spam_temp_probit.columns[1:]:\n",
    "        formula += f\" + {column}\"\n",
    "    \n",
    "    myprobit = sm.Probit.from_formula(formula, data=spam_temp_probit).fit()\n",
    "    p_probit = myprobit.pvalues\n",
    "    print(myprobit.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a9345",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_logit = mylogit.pvalues\n",
    "spam_temp_logit = spam_previous_words.copy()\n",
    "\n",
    "while any(p_logit > 0.05):\n",
    "    worstp = p_logit.idxmax()\n",
    "    \n",
    "    print(worstp)\n",
    "    spam_temp_logit.drop(columns=worstp, inplace=True)\n",
    "    \n",
    "    formula = \"spam ~\"\n",
    "    \n",
    "    for column in spam_temp_logit.columns[1:]:\n",
    "        formula += f\" + {column}\"\n",
    "    \n",
    "    mylogit = sm.Logit.from_formula(formula, data=spam_temp_logit).fit()\n",
    "    p_logit = mylogit.pvalues\n",
    "    print(mylogit.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce166104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linktest for probit model - after stepwise regression\n",
    "linktest_result_logit = linktest_logit(mylogit)\n",
    "print(linktest_result_logit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ff1c1f2",
   "metadata": {},
   "source": [
    "### Interaction terms\n",
    "---\n",
    "We include the interaction terms from the previous research. The variables that were discarded due to low variances are excluded from the interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_previous_words.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c6a4587",
   "metadata": {},
   "source": [
    "### Logit\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_interactions = \"spam ~ word_freq_make * word_freq_our  +  word_freq_free + word_freq_meeting  + word_freq_edu +  word_freq_semicolon * word_freq_exclamation * word_freq_hashtag\"\n",
    "\n",
    "mylogit = sm.formula.glm(formula_interactions, data=spam_previous_words, family=sm.families.Binomial(sm.genmod.families.links.logit())).fit()\n",
    "\n",
    "p = mylogit.pvalues\n",
    "print(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ce506e2",
   "metadata": {},
   "source": [
    "i = 1\n",
    "while any(p > 0.05):\n",
    "    worstp = p.idxmax()\n",
    "    print(worstp)\n",
    "\n",
    "    if i == 1:\n",
    "        # Remove the outcome variable from the formula\n",
    "        formula_interactions = formula_interactions.replace(\"spam ~ \", \"\")\n",
    "\n",
    "        # Create the design matrix with interaction terms\n",
    "        X = patsy.dmatrix(formula_interactions, data=spam_previous_words)\n",
    "\n",
    "        # Convert the design matrix to a DataFrame\n",
    "        X = pd.DataFrame(X, columns=X.design_info.column_names)\n",
    "        i=2\n",
    "    else:\n",
    "        X = X.drop(worstp, axis=1)\n",
    "        X_names = ['Intercept'] + list(X.columns)[1:]\n",
    "        X.columns = X_names\n",
    "\n",
    "        mylogit = sm.GLM(spam['spam'], X, family=sm.families.Binomial(sm.families.links.logit())).fit()\n",
    "\n",
    "        print(mylogit.summary())\n",
    "        p = mylogit.pvalues\n",
    "        print(mylogit.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa37ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_logit = sm.Logit(spam[\"spam\"], sm.add_constant(pd.Series([1] * len(spam_previous_words)))).fit()\n",
    "logit_lrtest = stats.chi2.sf(2 * (mylogit.llf - null_logit.llf), 1)\n",
    "print(\"Logit likelihood ratio test p-value:\", logit_lrtest)\n",
    "\n",
    "linktest_result_logit = linktest_probit(mylogit)\n",
    "print(\"LinkTest result\", linktest_result_logit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a31d57b",
   "metadata": {},
   "source": [
    "### Probit\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59f59531",
   "metadata": {},
   "source": [
    "formula_interactions = \"spam ~ word_freq_make * word_freq_our  +  word_freq_free + word_freq_meeting  + word_freq_edu +  word_freq_semicolon * word_freq_exclamation * word_freq_hashtag\"\n",
    "myprobit = sm.formula.glm(formula_interactions, data=spam_previous_words, family=sm.families.Binomial(sm.genmod.families.links.probit())).fit()\n",
    "\n",
    "p = myprobit.pvalues\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae79f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "while any(p > 0.05):\n",
    "    worstp = p.idxmax()\n",
    "    print(worstp)\n",
    "\n",
    "    if i == 1:\n",
    "        # Remove the outcome variable from the formula\n",
    "        formula_interactions = formula_interactions.replace(\"spam ~ \", \"\")\n",
    "\n",
    "        # Create the design matrix with interaction terms\n",
    "        X = patsy.dmatrix(formula_interactions, data=spam_previous_words)\n",
    "\n",
    "        # Convert the design matrix to a DataFrame\n",
    "        X = pd.DataFrame(X, columns=X.design_info.column_names)\n",
    "        i=2\n",
    "    else:\n",
    "        X = X.drop(worstp, axis=1)\n",
    "        X_names = ['Intercept'] + list(X.columns)[1:]\n",
    "        X.columns = X_names\n",
    "\n",
    "        myprobit = sm.GLM(spam['spam'], X, family=sm.families.Binomial(sm.families.links.probit())).fit()\n",
    "\n",
    "        print(myprobit.summary())\n",
    "        p = myprobit.pvalues\n",
    "        print(myprobit.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0db996",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_probit = sm.Probit(spam[\"spam\"], sm.add_constant(pd.Series([1] * len(spam_previous_words)))).fit()\n",
    "probit_lrtest = stats.chi2.sf(2 * (myprobit.llf - null_probit.llf), 1)\n",
    "print(\"Probitlikelihood ratio test p-value:\", probit_lrtest)\n",
    "\n",
    "linktest_result_probit = linktest_probit(myprobit)\n",
    "print(\"LinkTest result\", linktest_result_probit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c428d60c",
   "metadata": {},
   "source": [
    "### Other goodness of fit tests\n",
    "\n",
    "As in previous study the Link Test and LR test draw attention to the logit model with interactions that was good specified and significant\n",
    "Now let us perform other tests for the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb989191",
   "metadata": {},
   "source": [
    "gof_results = mylogit.deviance\n",
    "print(\"Logit | Goodness-of-Fit Test:\")\n",
    "print(\"Logit | Deviance: \", gof_results)\n",
    "\n",
    "gof_results = myprobit.deviance\n",
    "print(\"Probit | Goodness-of-Fit Test:\")\n",
    "print(\"Probit | Deviance: \", gof_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_r2 = 1 - (mylogit.llf / mylogit.llnull)\n",
    "print(\"Logit | Pseudo R-squared: \", pseudo_r2)\n",
    "\n",
    "pseudo_r2 = 1 - (myprobit.llf / myprobit.llnull)\n",
    "print(\"Probit | Pseudo R-squared: \", pseudo_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40378099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating marginal effects\n",
    "meff = smf.logit( formula_interactions, data=spam_previous_words).fit()\n",
    "print(\"Logit | Marginal Effects:\")\n",
    "print(meff.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating marginal effects\n",
    "meff = smf.probit( formula_interactions, data=spam_previous_words).fit()\n",
    "print(\"Probit | Marginal Effects:\")\n",
    "print(meff.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
